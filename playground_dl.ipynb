{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([200, 2048, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from deep_learning.models.feature_extractors.resnet import Resnet50\n",
    "\n",
    "model= Resnet50()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "random_input = torch.randn(200, 3, 256, 256).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    features = model(random_input)\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.julia\\conda\\3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\.julia\\conda\\3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B1_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B1_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([200, 1280, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from deep_learning.models.feature_extractors.efficientnet import EfficientNetB1\n",
    "\n",
    "model= EfficientNetB1()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "random_input = torch.randn(200, 3, 256, 256).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    features = model(random_input)\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.julia\\conda\\3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\.julia\\conda\\3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from deep_learning.feature_extraction import extract_features\n",
    "from deep_learning.models.transforms.transforms import transform_resnet\n",
    "from deep_learning.models.feature_extractors.resnet import Resnet50\n",
    "from deep_learning.loaders.image_batch_loader import load_batch_from_dir\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=Resnet50().to(device)\n",
    "\n",
    "features=extract_features(model,transform_resnet,\"F:/test/0005f7aaab2800f6170c399693a96917/tiles\")\n",
    "torch.save(features,\"F:/test/0005f7aaab2800f6170c399693a96917.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'features/img1.npy'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame (assuming it’s loaded from a CSV)\n",
    "df = pd.DataFrame({\n",
    "    \"feature_path\": [\"features/img1.npy\", \"features/img2.npy\", \"features/img3.npy\"],\n",
    "    \"label\": [0, 1, 0]\n",
    "})\n",
    "df[\"feature_path\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 760\n",
      "Test size: 191\n",
      "Train batches: 760\n",
      "Batch X shape: torch.Size([193, 2048])\n",
      "Batch Y shape: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "from deep_learning.loaders.feature_dataset import FeatureDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "df=pd.read_csv(\"F:/data/small.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Test size:\", len(test_df))\n",
    "\n",
    "dataset=FeatureDataset(train_df,\"F:/extracted_features\")\n",
    "\n",
    "train_loader = DataLoader(dataset, shuffle=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "\n",
    "for batch in train_loader:\n",
    "    x_batch, y_batch = batch\n",
    "    x_batch=x_batch.squeeze()\n",
    "    print(\"Batch X shape:\", x_batch.shape)  # Expected: (32, D)\n",
    "    print(\"Batch Y shape:\", y_batch.squeeze().shape)  # Expected: (32,)\n",
    "    break  # Only one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Clam                                     [1, 5]                    5,130\n",
       "├─Sequential: 1-1                        [100, 5]                  --\n",
       "│    └─Linear: 2-1                       [100, 512]                1,049,088\n",
       "│    └─ReLU: 2-2                         [100, 512]                --\n",
       "│    └─Dropout: 2-3                      [100, 512]                --\n",
       "│    └─Attn_Net_Gated: 2-4               [100, 5]                  --\n",
       "│    │    └─Sequential: 3-1              [100, 256]                131,328\n",
       "│    │    └─Sequential: 3-2              [100, 256]                131,328\n",
       "│    │    └─Linear: 3-3                  [100, 5]                  1,285\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─Linear: 2-5                       [1]                       513\n",
       "│    └─Linear: 2-6                       [1]                       513\n",
       "│    └─Linear: 2-7                       [1]                       513\n",
       "│    └─Linear: 2-8                       [1]                       513\n",
       "│    └─Linear: 2-9                       [1]                       513\n",
       "==========================================================================================\n",
       "Total params: 1,320,724\n",
       "Trainable params: 1,320,724\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 131.31\n",
       "==========================================================================================\n",
       "Input size (MB): 0.82\n",
       "Forward/backward pass size (MB): 0.82\n",
       "Params size (MB): 5.26\n",
       "Estimated Total Size (MB): 6.90\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deep_learning.models.attention_core.clam import Clam\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=Clam(\n",
    "    feature_vector_length=2048,\n",
    "    dropout=0.1,\n",
    "    k_sample=8,\n",
    "    n_classes=5,\n",
    "    subtyping=True\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "summary(model, input_size=(100,2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0363, -0.0058,  0.0846, -0.0593,  0.0797]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    x_batch, y_batch = batch\n",
    "    x_batch=x_batch.squeeze()\n",
    "    x_batch.to(device)\n",
    "    logits, Y_prob, Y_hat, A_raw, results_dict=model(x_batch)\n",
    "    print(logits)\n",
    "    break  # Only one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "batch 19, loss: 2.6146, instance_loss: 0.7178, weighted_loss: 1.6662, label: 2, bag_size: 198\n",
      "batch 39, loss: 1.7910, instance_loss: 0.6853, weighted_loss: 1.2382, label: 5, bag_size: 145\n",
      "batch 59, loss: 2.4337, instance_loss: 0.7029, weighted_loss: 1.5683, label: 3, bag_size: 128\n",
      "batch 79, loss: 1.0695, instance_loss: 0.5800, weighted_loss: 0.8247, label: 1, bag_size: 395\n",
      "batch 99, loss: 3.1832, instance_loss: 0.6473, weighted_loss: 1.9152, label: 2, bag_size: 184\n",
      "batch 119, loss: 1.7686, instance_loss: 0.5912, weighted_loss: 1.1799, label: 1, bag_size: 246\n",
      "batch 139, loss: 2.1358, instance_loss: 0.6433, weighted_loss: 1.3895, label: 3, bag_size: 167\n",
      "batch 159, loss: 1.2676, instance_loss: 0.3248, weighted_loss: 0.7962, label: 0, bag_size: 236\n",
      "batch 179, loss: 2.4001, instance_loss: 0.8914, weighted_loss: 1.6457, label: 5, bag_size: 134\n",
      "batch 199, loss: 2.4610, instance_loss: 0.6065, weighted_loss: 1.5337, label: 2, bag_size: 225\n",
      "batch 219, loss: 1.2204, instance_loss: 0.5954, weighted_loss: 0.9079, label: 1, bag_size: 251\n",
      "batch 239, loss: 1.2330, instance_loss: 0.1758, weighted_loss: 0.7044, label: 0, bag_size: 208\n",
      "batch 259, loss: 0.9294, instance_loss: 0.2110, weighted_loss: 0.5702, label: 1, bag_size: 166\n",
      "batch 279, loss: 2.5239, instance_loss: 0.5973, weighted_loss: 1.5606, label: 2, bag_size: 290\n",
      "batch 299, loss: 1.2293, instance_loss: 0.2009, weighted_loss: 0.7151, label: 1, bag_size: 201\n",
      "batch 319, loss: 1.8755, instance_loss: 0.7739, weighted_loss: 1.3247, label: 0, bag_size: 89\n",
      "batch 339, loss: 2.3296, instance_loss: 0.5998, weighted_loss: 1.4647, label: 2, bag_size: 186\n",
      "batch 359, loss: 1.1174, instance_loss: 0.0736, weighted_loss: 0.5955, label: 1, bag_size: 151\n",
      "batch 379, loss: 1.8497, instance_loss: 0.2124, weighted_loss: 1.0311, label: 3, bag_size: 150\n",
      "batch 399, loss: 2.3139, instance_loss: 0.3192, weighted_loss: 1.3165, label: 4, bag_size: 154\n",
      "batch 419, loss: 1.1358, instance_loss: 0.0632, weighted_loss: 0.5995, label: 1, bag_size: 148\n",
      "batch 439, loss: 2.2055, instance_loss: 0.1915, weighted_loss: 1.1985, label: 4, bag_size: 315\n",
      "batch 459, loss: 1.0940, instance_loss: 0.0345, weighted_loss: 0.5643, label: 1, bag_size: 259\n",
      "batch 479, loss: 1.7632, instance_loss: 0.3537, weighted_loss: 1.0584, label: 5, bag_size: 187\n",
      "batch 499, loss: 2.1493, instance_loss: 0.2486, weighted_loss: 1.1990, label: 5, bag_size: 234\n",
      "batch 519, loss: 1.4090, instance_loss: 1.1365, weighted_loss: 1.2728, label: 5, bag_size: 113\n",
      "batch 539, loss: 1.3623, instance_loss: 0.2935, weighted_loss: 0.8279, label: 0, bag_size: 147\n",
      "batch 559, loss: 1.6447, instance_loss: 0.1355, weighted_loss: 0.8901, label: 4, bag_size: 185\n",
      "batch 579, loss: 2.0042, instance_loss: 0.3067, weighted_loss: 1.1555, label: 2, bag_size: 176\n",
      "batch 599, loss: 0.8147, instance_loss: 0.0434, weighted_loss: 0.4290, label: 0, bag_size: 155\n",
      "Epoch 0 Summary: train_loss: 1.6744, train_clustering_loss:  0.4316, train_accuracy: 0.2549\n",
      "Slide Accuracy:\n",
      "class 0: acc 0.39634146341463417, correct 65/164\n",
      "class 1: acc 0.42105263157894735, correct 72/171\n",
      "class 2: acc 0.027777777777777776, correct 2/72\n",
      "class 3: acc 0.0, correct 0/61\n",
      "class 4: acc 0.05714285714285714, correct 4/70\n",
      "class 5: acc 0.17142857142857143, correct 12/70\n",
      "Patch Accuracy:\n",
      "class 0: acc 0.8715049342105263, correct 4239/4864\n",
      "class 1: acc 0.7569901315789473, correct 3682/4864\n",
      "class 2: acc None, correct 0/0\n",
      "class 3: acc None, correct 0/0\n",
      "class 4: acc None, correct 0/0\n",
      "class 5: acc None, correct 0/0\n",
      "[[65. 78. 11.  1.  2.  7.]\n",
      " [76. 72. 11.  1.  3.  8.]\n",
      " [28. 37.  2.  0.  1.  4.]\n",
      " [13. 38.  3.  0.  2.  5.]\n",
      " [13. 39.  3.  0.  4. 11.]\n",
      " [23. 30.  2.  0.  3. 12.]]\n",
      "\n",
      "\n",
      "batch 19, loss: 1.7901, instance_loss: 0.6936, weighted_loss: 1.2418, label: 1, bag_size: 90\n",
      "batch 39, loss: 2.1667, instance_loss: 0.2273, weighted_loss: 1.1970, label: 2, bag_size: 153\n",
      "batch 59, loss: 1.2636, instance_loss: 0.9117, weighted_loss: 1.0876, label: 4, bag_size: 9\n",
      "batch 79, loss: 0.8532, instance_loss: 0.0343, weighted_loss: 0.4437, label: 0, bag_size: 180\n",
      "batch 99, loss: 4.7571, instance_loss: 0.2322, weighted_loss: 2.4947, label: 5, bag_size: 266\n",
      "batch 119, loss: 1.3997, instance_loss: 0.0329, weighted_loss: 0.7163, label: 1, bag_size: 212\n",
      "batch 139, loss: 0.9372, instance_loss: 0.7659, weighted_loss: 0.8516, label: 5, bag_size: 152\n",
      "batch 159, loss: 1.1064, instance_loss: 0.0280, weighted_loss: 0.5672, label: 1, bag_size: 291\n",
      "batch 179, loss: 2.7680, instance_loss: 0.5752, weighted_loss: 1.6716, label: 5, bag_size: 197\n",
      "batch 199, loss: 1.8825, instance_loss: 0.3852, weighted_loss: 1.1338, label: 1, bag_size: 164\n",
      "batch 219, loss: 1.3357, instance_loss: 0.0149, weighted_loss: 0.6753, label: 1, bag_size: 221\n",
      "batch 239, loss: 2.8108, instance_loss: 0.1165, weighted_loss: 1.4636, label: 3, bag_size: 162\n",
      "batch 259, loss: 2.4455, instance_loss: 0.5041, weighted_loss: 1.4748, label: 5, bag_size: 126\n",
      "batch 279, loss: 0.5323, instance_loss: 0.0486, weighted_loss: 0.2904, label: 0, bag_size: 89\n",
      "batch 299, loss: 0.8538, instance_loss: 0.0951, weighted_loss: 0.4745, label: 0, bag_size: 214\n",
      "batch 319, loss: 2.2592, instance_loss: 0.3092, weighted_loss: 1.2842, label: 2, bag_size: 175\n",
      "batch 339, loss: 1.0405, instance_loss: 0.0614, weighted_loss: 0.5509, label: 1, bag_size: 159\n",
      "batch 359, loss: 0.8743, instance_loss: 0.1382, weighted_loss: 0.5062, label: 1, bag_size: 141\n",
      "batch 379, loss: 1.1449, instance_loss: 0.0339, weighted_loss: 0.5894, label: 1, bag_size: 222\n",
      "batch 399, loss: 1.5279, instance_loss: 0.2104, weighted_loss: 0.8691, label: 5, bag_size: 158\n",
      "batch 419, loss: 2.0672, instance_loss: 0.3214, weighted_loss: 1.1943, label: 2, bag_size: 186\n",
      "batch 439, loss: 0.9771, instance_loss: 0.1094, weighted_loss: 0.5432, label: 0, bag_size: 170\n",
      "batch 459, loss: 0.9397, instance_loss: 0.0227, weighted_loss: 0.4812, label: 1, bag_size: 193\n",
      "batch 479, loss: 3.0415, instance_loss: 0.0792, weighted_loss: 1.5603, label: 3, bag_size: 216\n",
      "batch 499, loss: 1.1370, instance_loss: 0.3001, weighted_loss: 0.7185, label: 1, bag_size: 125\n",
      "batch 519, loss: 2.4884, instance_loss: 0.6609, weighted_loss: 1.5747, label: 2, bag_size: 195\n",
      "batch 539, loss: 1.8042, instance_loss: 0.2967, weighted_loss: 1.0505, label: 3, bag_size: 211\n",
      "batch 559, loss: 2.0103, instance_loss: 0.2645, weighted_loss: 1.1374, label: 2, bag_size: 184\n",
      "batch 579, loss: 1.4322, instance_loss: 0.1993, weighted_loss: 0.8157, label: 1, bag_size: 141\n",
      "batch 599, loss: 0.8992, instance_loss: 0.1361, weighted_loss: 0.5176, label: 1, bag_size: 114\n",
      "Epoch 1 Summary: train_loss: 1.5550, train_clustering_loss:  0.2799, train_accuracy: 0.3487\n",
      "Slide Accuracy:\n",
      "class 0: acc 0.49390243902439024, correct 81/164\n",
      "class 1: acc 0.5029239766081871, correct 86/171\n",
      "class 2: acc 0.027777777777777776, correct 2/72\n",
      "class 3: acc 0.03278688524590164, correct 2/61\n",
      "class 4: acc 0.18571428571428572, correct 13/70\n",
      "class 5: acc 0.4, correct 28/70\n",
      "Patch Accuracy:\n",
      "class 0: acc 0.9076891447368421, correct 4415/4864\n",
      "class 1: acc 0.8682154605263158, correct 4223/4864\n",
      "class 2: acc None, correct 0/0\n",
      "class 3: acc None, correct 0/0\n",
      "class 4: acc None, correct 0/0\n",
      "class 5: acc None, correct 0/0\n",
      "[[81. 57.  5.  5. 12.  4.]\n",
      " [63. 86.  5.  1. 11.  5.]\n",
      " [18. 40.  2.  2.  7.  3.]\n",
      " [ 9. 29.  3.  2. 11.  7.]\n",
      " [15. 16.  4.  3. 13. 19.]\n",
      " [14. 13.  1.  4. 10. 28.]]\n",
      "\n",
      "\n",
      "batch 19, loss: 0.9869, instance_loss: 0.0368, weighted_loss: 0.5119, label: 1, bag_size: 275\n",
      "batch 39, loss: 2.0860, instance_loss: 0.1016, weighted_loss: 1.0938, label: 4, bag_size: 187\n",
      "batch 59, loss: 1.7602, instance_loss: 0.0513, weighted_loss: 0.9058, label: 4, bag_size: 160\n",
      "batch 79, loss: 1.4759, instance_loss: 0.1233, weighted_loss: 0.7996, label: 1, bag_size: 188\n",
      "batch 99, loss: 1.4619, instance_loss: 0.6477, weighted_loss: 1.0548, label: 3, bag_size: 115\n",
      "batch 119, loss: 0.8315, instance_loss: 0.2265, weighted_loss: 0.5290, label: 0, bag_size: 257\n",
      "batch 139, loss: 0.4347, instance_loss: 0.1691, weighted_loss: 0.3019, label: 0, bag_size: 66\n",
      "batch 159, loss: 0.5454, instance_loss: 0.0564, weighted_loss: 0.3009, label: 5, bag_size: 150\n",
      "batch 179, loss: 0.7364, instance_loss: 0.1091, weighted_loss: 0.4227, label: 0, bag_size: 104\n",
      "batch 199, loss: 1.4602, instance_loss: 0.4309, weighted_loss: 0.9456, label: 4, bag_size: 242\n",
      "batch 219, loss: 2.3760, instance_loss: 0.1292, weighted_loss: 1.2526, label: 2, bag_size: 123\n",
      "batch 239, loss: 0.6842, instance_loss: 0.1169, weighted_loss: 0.4005, label: 0, bag_size: 139\n",
      "batch 259, loss: 2.2720, instance_loss: 0.1673, weighted_loss: 1.2197, label: 2, bag_size: 147\n",
      "batch 279, loss: 2.0188, instance_loss: 0.3473, weighted_loss: 1.1831, label: 3, bag_size: 188\n",
      "batch 299, loss: 1.0261, instance_loss: 0.0370, weighted_loss: 0.5316, label: 5, bag_size: 125\n",
      "batch 319, loss: 1.1822, instance_loss: 0.1723, weighted_loss: 0.6772, label: 3, bag_size: 145\n",
      "batch 339, loss: 1.6081, instance_loss: 0.3703, weighted_loss: 0.9892, label: 0, bag_size: 36\n",
      "batch 359, loss: 1.6310, instance_loss: 0.0766, weighted_loss: 0.8538, label: 5, bag_size: 57\n",
      "batch 379, loss: 0.9923, instance_loss: 0.0125, weighted_loss: 0.5024, label: 1, bag_size: 302\n",
      "batch 399, loss: 1.0527, instance_loss: 0.1498, weighted_loss: 0.6012, label: 0, bag_size: 240\n",
      "batch 419, loss: 1.1731, instance_loss: 0.1532, weighted_loss: 0.6631, label: 0, bag_size: 196\n",
      "batch 439, loss: 0.4995, instance_loss: 0.0225, weighted_loss: 0.2610, label: 1, bag_size: 395\n",
      "batch 459, loss: 2.1368, instance_loss: 0.0807, weighted_loss: 1.1087, label: 0, bag_size: 103\n",
      "batch 479, loss: 0.8104, instance_loss: 0.0194, weighted_loss: 0.4149, label: 1, bag_size: 235\n",
      "batch 499, loss: 2.0539, instance_loss: 0.2329, weighted_loss: 1.1434, label: 2, bag_size: 184\n",
      "batch 519, loss: 5.8239, instance_loss: 0.0807, weighted_loss: 2.9523, label: 3, bag_size: 150\n",
      "batch 539, loss: 0.8892, instance_loss: 0.0652, weighted_loss: 0.4772, label: 0, bag_size: 154\n",
      "batch 559, loss: 0.6772, instance_loss: 0.0574, weighted_loss: 0.3673, label: 0, bag_size: 214\n",
      "batch 579, loss: 2.1410, instance_loss: 0.4939, weighted_loss: 1.3175, label: 4, bag_size: 40\n",
      "batch 599, loss: 1.0773, instance_loss: 0.0737, weighted_loss: 0.5755, label: 0, bag_size: 153\n",
      "Epoch 2 Summary: train_loss: 1.4895, train_clustering_loss:  0.1997, train_accuracy: 0.3914\n",
      "Slide Accuracy:\n",
      "class 0: acc 0.5792682926829268, correct 95/164\n",
      "class 1: acc 0.52046783625731, correct 89/171\n",
      "class 2: acc 0.0, correct 0/72\n",
      "class 3: acc 0.11475409836065574, correct 7/61\n",
      "class 4: acc 0.17142857142857143, correct 12/70\n",
      "class 5: acc 0.5, correct 35/70\n",
      "Patch Accuracy:\n",
      "class 0: acc 0.9391447368421053, correct 4568/4864\n",
      "class 1: acc 0.9159128289473685, correct 4455/4864\n",
      "class 2: acc None, correct 0/0\n",
      "class 3: acc None, correct 0/0\n",
      "class 4: acc None, correct 0/0\n",
      "class 5: acc None, correct 0/0\n",
      "[[95. 51.  0.  1.  9.  8.]\n",
      " [68. 89.  0.  7.  6.  1.]\n",
      " [18. 38.  0.  4.  7.  5.]\n",
      " [ 9. 18.  0.  7. 13. 14.]\n",
      " [10. 25.  0.  7. 12. 16.]\n",
      " [11. 10.  0.  7.  7. 35.]]\n"
     ]
    }
   ],
   "source": [
    "from deep_learning.loaders.feature_dataset import FeatureDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from deep_learning.train.train import train\n",
    "from deep_learning.models.attention_core.clam import Clam\n",
    "\n",
    "df=pd.read_csv(\"F:/data/small.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "model=Clam(\n",
    "    feature_vector_length=2048,\n",
    "    dropout=0.1,\n",
    "    k_sample=8,\n",
    "    n_classes=6,\n",
    "    subtyping=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train(model,train_df,\"F:/extracted_features\",3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
