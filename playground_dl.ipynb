{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([200, 2048, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from deep_learning.models.feature_extractors.resnet import Resnet50\n",
    "\n",
    "model= Resnet50()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "random_input = torch.randn(200, 3, 256, 256).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    features = model(random_input)\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.julia\\conda\\3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\.julia\\conda\\3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B1_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B1_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([200, 1280, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from deep_learning.models.feature_extractors.efficientnet import EfficientNetB1\n",
    "\n",
    "model= EfficientNetB1()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "random_input = torch.randn(200, 3, 256, 256).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    features = model(random_input)\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.julia\\conda\\3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\.julia\\conda\\3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from deep_learning.feature_extraction import extract_features\n",
    "from deep_learning.models.transforms.transforms import transform_resnet\n",
    "from deep_learning.models.feature_extractors.resnet import Resnet50\n",
    "from deep_learning.loaders.image_batch_loader import load_batch_from_dir\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=Resnet50().to(device)\n",
    "\n",
    "features=extract_features(model,transform_resnet,\"F:/test/0005f7aaab2800f6170c399693a96917/tiles\")\n",
    "torch.save(features,\"F:/test/0005f7aaab2800f6170c399693a96917.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'features/img1.npy'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame (assuming it’s loaded from a CSV)\n",
    "df = pd.DataFrame({\n",
    "    \"feature_path\": [\"features/img1.npy\", \"features/img2.npy\", \"features/img3.npy\"],\n",
    "    \"label\": [0, 1, 0]\n",
    "})\n",
    "df[\"feature_path\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 760\n",
      "Test size: 191\n",
      "Train batches: 760\n",
      "Batch X shape: torch.Size([193, 2048])\n",
      "Batch Y shape: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "from deep_learning.loaders.feature_dataset import FeatureDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "df=pd.read_csv(\"F:/data/small.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Test size:\", len(test_df))\n",
    "\n",
    "dataset=FeatureDataset(train_df,\"F:/extracted_features\")\n",
    "\n",
    "train_loader = DataLoader(dataset, shuffle=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "\n",
    "for batch in train_loader:\n",
    "    x_batch, y_batch = batch\n",
    "    x_batch=x_batch.squeeze()\n",
    "    print(\"Batch X shape:\", x_batch.shape)  # Expected: (32, D)\n",
    "    print(\"Batch Y shape:\", y_batch.squeeze().shape)  # Expected: (32,)\n",
    "    break  # Only one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Clam                                     [1, 5]                    5,130\n",
       "├─Sequential: 1-1                        [100, 5]                  --\n",
       "│    └─Linear: 2-1                       [100, 512]                1,049,088\n",
       "│    └─ReLU: 2-2                         [100, 512]                --\n",
       "│    └─Dropout: 2-3                      [100, 512]                --\n",
       "│    └─Attn_Net_Gated: 2-4               [100, 5]                  --\n",
       "│    │    └─Sequential: 3-1              [100, 256]                131,328\n",
       "│    │    └─Sequential: 3-2              [100, 256]                131,328\n",
       "│    │    └─Linear: 3-3                  [100, 5]                  1,285\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─Linear: 2-5                       [1]                       513\n",
       "│    └─Linear: 2-6                       [1]                       513\n",
       "│    └─Linear: 2-7                       [1]                       513\n",
       "│    └─Linear: 2-8                       [1]                       513\n",
       "│    └─Linear: 2-9                       [1]                       513\n",
       "==========================================================================================\n",
       "Total params: 1,320,724\n",
       "Trainable params: 1,320,724\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 131.31\n",
       "==========================================================================================\n",
       "Input size (MB): 0.82\n",
       "Forward/backward pass size (MB): 0.82\n",
       "Params size (MB): 5.26\n",
       "Estimated Total Size (MB): 6.90\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deep_learning.models.attention_core.clam import Clam\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=Clam(\n",
    "    feature_vector_length=2048,\n",
    "    dropout=0.1,\n",
    "    k_sample=8,\n",
    "    n_classes=5,\n",
    "    subtyping=True\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "summary(model, input_size=(100,2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0363, -0.0058,  0.0846, -0.0593,  0.0797]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    x_batch, y_batch = batch\n",
    "    x_batch=x_batch.squeeze()\n",
    "    x_batch.to(device)\n",
    "    logits, Y_prob, Y_hat, A_raw, results_dict=model(x_batch)\n",
    "    print(logits)\n",
    "    break  # Only one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "batch 19, loss: 3.1088, instance_loss: 0.6831, weighted_loss: 1.8960, label: 5, bag_size: 134\n",
      "batch 39, loss: 1.3058, instance_loss: 0.7195, weighted_loss: 1.0126, label: 0, bag_size: 65\n",
      "batch 59, loss: 0.9766, instance_loss: 0.6696, weighted_loss: 0.8231, label: 0, bag_size: 32\n",
      "batch 79, loss: 1.2730, instance_loss: 0.6783, weighted_loss: 0.9756, label: 1, bag_size: 232\n",
      "batch 99, loss: 1.6767, instance_loss: 0.6514, weighted_loss: 1.1640, label: 1, bag_size: 229\n",
      "batch 119, loss: 1.5221, instance_loss: 0.6370, weighted_loss: 1.0795, label: 1, bag_size: 111\n",
      "batch 139, loss: 1.0973, instance_loss: 0.6373, weighted_loss: 0.8673, label: 0, bag_size: 218\n",
      "batch 159, loss: 0.8908, instance_loss: 0.7877, weighted_loss: 0.8392, label: 0, bag_size: 174\n",
      "batch 179, loss: 0.9654, instance_loss: 0.7152, weighted_loss: 0.8403, label: 0, bag_size: 89\n",
      "batch 199, loss: 1.9491, instance_loss: 0.6436, weighted_loss: 1.2964, label: 4, bag_size: 162\n",
      "batch 219, loss: 1.4830, instance_loss: 0.2894, weighted_loss: 0.8862, label: 1, bag_size: 127\n",
      "batch 239, loss: 1.7390, instance_loss: 0.6580, weighted_loss: 1.1985, label: 5, bag_size: 177\n",
      "batch 259, loss: 1.4229, instance_loss: 0.5289, weighted_loss: 0.9759, label: 0, bag_size: 85\n",
      "batch 279, loss: 1.1677, instance_loss: 0.9217, weighted_loss: 1.0447, label: 0, bag_size: 154\n",
      "batch 299, loss: 1.4876, instance_loss: 0.7485, weighted_loss: 1.1181, label: 0, bag_size: 64\n",
      "batch 319, loss: 1.4839, instance_loss: 0.7692, weighted_loss: 1.1266, label: 0, bag_size: 147\n",
      "batch 339, loss: 0.9626, instance_loss: 0.0832, weighted_loss: 0.5229, label: 1, bag_size: 81\n",
      "batch 359, loss: 0.7492, instance_loss: 0.5072, weighted_loss: 0.6282, label: 0, bag_size: 153\n",
      "batch 379, loss: 2.4120, instance_loss: 0.5353, weighted_loss: 1.4736, label: 5, bag_size: 195\n",
      "batch 399, loss: 1.4163, instance_loss: 0.1408, weighted_loss: 0.7785, label: 1, bag_size: 122\n",
      "batch 419, loss: 2.7736, instance_loss: 0.4848, weighted_loss: 1.6292, label: 5, bag_size: 158\n",
      "batch 439, loss: 1.1213, instance_loss: 0.0292, weighted_loss: 0.5753, label: 1, bag_size: 189\n",
      "batch 459, loss: 2.5940, instance_loss: 0.1954, weighted_loss: 1.3947, label: 4, bag_size: 190\n",
      "batch 479, loss: 1.0977, instance_loss: 0.0157, weighted_loss: 0.5567, label: 1, bag_size: 115\n",
      "batch 499, loss: 2.2611, instance_loss: 1.0649, weighted_loss: 1.6630, label: 0, bag_size: 73\n",
      "batch 519, loss: 1.7658, instance_loss: 0.4560, weighted_loss: 1.1109, label: 2, bag_size: 172\n",
      "batch 539, loss: 1.7381, instance_loss: 0.3825, weighted_loss: 1.0603, label: 5, bag_size: 135\n",
      "batch 559, loss: 0.9600, instance_loss: 0.1853, weighted_loss: 0.5727, label: 0, bag_size: 202\n",
      "batch 579, loss: 1.7944, instance_loss: 0.0976, weighted_loss: 0.9460, label: 4, bag_size: 168\n",
      "batch 599, loss: 1.4636, instance_loss: 0.4179, weighted_loss: 0.9407, label: 5, bag_size: 224\n",
      "Epoch: 0, train_loss: 1.6443, train_clustering_loss:  0.4622, train_error: 0.7155\n",
      "class 0: acc 0.5365853658536586, correct 88/164\n",
      "class 1: acc 0.4152046783625731, correct 71/171\n",
      "class 2: acc 0.05555555555555555, correct 4/72\n",
      "class 3: acc 0.0, correct 0/61\n",
      "class 4: acc 0.08571428571428572, correct 6/70\n",
      "class 5: acc 0.05714285714285714, correct 4/70\n"
     ]
    }
   ],
   "source": [
    "from deep_learning.loaders.feature_dataset import FeatureDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from deep_learning.train.train import train\n",
    "from deep_learning.models.attention_core.clam import Clam\n",
    "\n",
    "df=pd.read_csv(\"F:/data/small.csv\")\n",
    "train_df, test_df = train_test_split(df[:1000], test_size=0.2, random_state=42)\n",
    "\n",
    "model=Clam(\n",
    "    feature_vector_length=2048,\n",
    "    dropout=0.1,\n",
    "    k_sample=8,\n",
    "    n_classes=6,\n",
    "    subtyping=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train(model,train_df,\"F:/extracted_features\",10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
