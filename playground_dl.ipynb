{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([200, 2048, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from deep_learning.models.feature_extractors.resnet import Resnet50\n",
    "\n",
    "model= Resnet50()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "random_input = torch.randn(200, 3, 256, 256).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    features = model(random_input)\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.julia\\conda\\3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\.julia\\conda\\3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B1_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B1_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([200, 1280, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from deep_learning.models.feature_extractors.efficientnet import EfficientNetB1\n",
    "\n",
    "model= EfficientNetB1()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "random_input = torch.randn(200, 3, 256, 256).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    features = model(random_input)\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.julia\\conda\\3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\.julia\\conda\\3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from deep_learning.feature_extraction import extract_features\n",
    "from deep_learning.models.transforms.transforms import transform_resnet\n",
    "from deep_learning.models.feature_extractors.resnet import Resnet50\n",
    "from deep_learning.loaders.image_batch_loader import load_batch_from_dir\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=Resnet50().to(device)\n",
    "\n",
    "features=extract_features(model,transform_resnet,\"F:/test/0005f7aaab2800f6170c399693a96917/tiles\")\n",
    "torch.save(features,\"F:/test/0005f7aaab2800f6170c399693a96917.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'features/img1.npy'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame (assuming it’s loaded from a CSV)\n",
    "df = pd.DataFrame({\n",
    "    \"feature_path\": [\"features/img1.npy\", \"features/img2.npy\", \"features/img3.npy\"],\n",
    "    \"label\": [0, 1, 0]\n",
    "})\n",
    "df[\"feature_path\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 760\n",
      "Test size: 191\n",
      "Train batches: 760\n",
      "Batch X shape: torch.Size([193, 2048])\n",
      "Batch Y shape: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "from deep_learning.loaders.feature_dataset import FeatureDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "df=pd.read_csv(\"F:/data/small.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Test size:\", len(test_df))\n",
    "\n",
    "dataset=FeatureDataset(train_df,\"F:/extracted_features\")\n",
    "\n",
    "train_loader = DataLoader(dataset, shuffle=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "\n",
    "for batch in train_loader:\n",
    "    x_batch, y_batch = batch\n",
    "    x_batch=x_batch.squeeze()\n",
    "    print(\"Batch X shape:\", x_batch.shape)  # Expected: (32, D)\n",
    "    print(\"Batch Y shape:\", y_batch.squeeze().shape)  # Expected: (32,)\n",
    "    break  # Only one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Clam                                     [1, 5]                    5,130\n",
       "├─Sequential: 1-1                        [100, 5]                  --\n",
       "│    └─Linear: 2-1                       [100, 512]                1,049,088\n",
       "│    └─ReLU: 2-2                         [100, 512]                --\n",
       "│    └─Dropout: 2-3                      [100, 512]                --\n",
       "│    └─Attn_Net_Gated: 2-4               [100, 5]                  --\n",
       "│    │    └─Sequential: 3-1              [100, 256]                131,328\n",
       "│    │    └─Sequential: 3-2              [100, 256]                131,328\n",
       "│    │    └─Linear: 3-3                  [100, 5]                  1,285\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─Linear: 2-5                       [1]                       513\n",
       "│    └─Linear: 2-6                       [1]                       513\n",
       "│    └─Linear: 2-7                       [1]                       513\n",
       "│    └─Linear: 2-8                       [1]                       513\n",
       "│    └─Linear: 2-9                       [1]                       513\n",
       "==========================================================================================\n",
       "Total params: 1,320,724\n",
       "Trainable params: 1,320,724\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 131.31\n",
       "==========================================================================================\n",
       "Input size (MB): 0.82\n",
       "Forward/backward pass size (MB): 0.82\n",
       "Params size (MB): 5.26\n",
       "Estimated Total Size (MB): 6.90\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deep_learning.models.attention_core.clam import Clam\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=Clam(\n",
    "    feature_vector_length=2048,\n",
    "    dropout=0.1,\n",
    "    k_sample=8,\n",
    "    n_classes=5,\n",
    "    subtyping=True\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "summary(model, input_size=(100,2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0363, -0.0058,  0.0846, -0.0593,  0.0797]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    x_batch, y_batch = batch\n",
    "    x_batch=x_batch.squeeze()\n",
    "    x_batch.to(device)\n",
    "    logits, Y_prob, Y_hat, A_raw, results_dict=model(x_batch)\n",
    "    print(logits)\n",
    "    break  # Only one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "batch 19, loss: 1.3340, instance_loss: 0.8248, weighted_loss: 1.0794, label: 1, bag_size: 237\n",
      "batch 39, loss: 2.2372, instance_loss: 0.1850, weighted_loss: 1.2111, label: 5, bag_size: 185\n",
      "batch 59, loss: 3.0818, instance_loss: 0.1688, weighted_loss: 1.6253, label: 3, bag_size: 162\n",
      "batch 79, loss: 1.4048, instance_loss: 1.0536, weighted_loss: 1.2292, label: 1, bag_size: 154\n",
      "batch 99, loss: 1.1544, instance_loss: 0.1886, weighted_loss: 0.6715, label: 0, bag_size: 277\n",
      "batch 119, loss: 2.2036, instance_loss: 0.2840, weighted_loss: 1.2438, label: 2, bag_size: 225\n",
      "batch 139, loss: 1.9374, instance_loss: 0.1668, weighted_loss: 1.0521, label: 5, bag_size: 176\n",
      "batch 159, loss: 2.7420, instance_loss: 0.1708, weighted_loss: 1.4564, label: 2, bag_size: 197\n",
      "batch 179, loss: 2.1308, instance_loss: 0.2118, weighted_loss: 1.1713, label: 2, bag_size: 103\n",
      "batch 199, loss: 1.2865, instance_loss: 0.2050, weighted_loss: 0.7457, label: 0, bag_size: 175\n",
      "batch 219, loss: 1.0309, instance_loss: 0.9467, weighted_loss: 0.9888, label: 1, bag_size: 114\n",
      "batch 239, loss: 1.6601, instance_loss: 1.0344, weighted_loss: 1.3473, label: 1, bag_size: 140\n",
      "batch 259, loss: 1.9458, instance_loss: 0.1301, weighted_loss: 1.0380, label: 2, bag_size: 168\n",
      "batch 279, loss: 1.9218, instance_loss: 0.1361, weighted_loss: 1.0290, label: 3, bag_size: 156\n",
      "batch 299, loss: 1.2498, instance_loss: 0.0994, weighted_loss: 0.6746, label: 0, bag_size: 147\n",
      "batch 319, loss: 1.8060, instance_loss: 0.1976, weighted_loss: 1.0018, label: 2, bag_size: 180\n",
      "batch 339, loss: 2.4054, instance_loss: 0.2264, weighted_loss: 1.3159, label: 3, bag_size: 164\n",
      "batch 359, loss: 2.1572, instance_loss: 0.2810, weighted_loss: 1.2191, label: 2, bag_size: 173\n",
      "batch 379, loss: 1.4085, instance_loss: 0.1783, weighted_loss: 0.7934, label: 5, bag_size: 197\n",
      "batch 399, loss: 1.7584, instance_loss: 0.1732, weighted_loss: 0.9658, label: 2, bag_size: 176\n",
      "batch 419, loss: 2.2422, instance_loss: 0.0964, weighted_loss: 1.1693, label: 3, bag_size: 145\n",
      "batch 439, loss: 1.5666, instance_loss: 0.1189, weighted_loss: 0.8427, label: 4, bag_size: 88\n",
      "batch 459, loss: 2.0583, instance_loss: 0.1797, weighted_loss: 1.1190, label: 5, bag_size: 187\n",
      "batch 479, loss: 1.8683, instance_loss: 0.2107, weighted_loss: 1.0395, label: 3, bag_size: 158\n",
      "batch 499, loss: 1.9646, instance_loss: 0.0956, weighted_loss: 1.0301, label: 2, bag_size: 155\n",
      "batch 519, loss: 1.2023, instance_loss: 0.9142, weighted_loss: 1.0582, label: 1, bag_size: 148\n",
      "batch 539, loss: 1.5095, instance_loss: 0.1779, weighted_loss: 0.8437, label: 0, bag_size: 123\n",
      "batch 559, loss: 1.6032, instance_loss: 0.1006, weighted_loss: 0.8519, label: 5, bag_size: 113\n",
      "batch 579, loss: 1.0689, instance_loss: 0.9201, weighted_loss: 0.9945, label: 1, bag_size: 117\n",
      "batch 599, loss: 1.2405, instance_loss: 0.9762, weighted_loss: 1.1084, label: 1, bag_size: 275\n",
      "Epoch: 0, train_loss: 1.6577, train_clustering_loss:  0.4238, train_error: 0.7204\n",
      "class 0: acc 0.4329268292682927, correct 71/164\n",
      "class 1: acc 0.4678362573099415, correct 80/171\n",
      "class 2: acc 0.0, correct 0/72\n",
      "class 3: acc 0.03278688524590164, correct 2/61\n",
      "class 4: acc 0.15714285714285714, correct 11/70\n",
      "class 5: acc 0.08571428571428572, correct 6/70\n",
      "\n",
      "Val Set, val_loss: 1.6675, val_error: 0.7303\n",
      "class 0: acc 0.7209302325581395, correct 31/43\n",
      "class 1: acc 0.05, correct 2/40\n",
      "class 2: acc 0.0, correct 0/25\n",
      "class 3: acc 0.0, correct 0/12\n",
      "class 4: acc 0.5333333333333333, correct 8/15\n",
      "class 5: acc 0.0, correct 0/17\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5949, instance_loss: 0.1880, weighted_loss: 0.3915, label: 0, bag_size: 475\n",
      "batch 39, loss: 1.7261, instance_loss: 0.1013, weighted_loss: 0.9137, label: 5, bag_size: 197\n",
      "batch 59, loss: 0.8786, instance_loss: 0.2657, weighted_loss: 0.5721, label: 0, bag_size: 188\n",
      "batch 79, loss: 1.7142, instance_loss: 0.1936, weighted_loss: 0.9539, label: 5, bag_size: 167\n",
      "batch 99, loss: 1.1505, instance_loss: 0.8087, weighted_loss: 0.9796, label: 1, bag_size: 246\n",
      "batch 119, loss: 1.1378, instance_loss: 0.8617, weighted_loss: 0.9998, label: 1, bag_size: 122\n",
      "batch 139, loss: 2.5786, instance_loss: 0.0677, weighted_loss: 1.3232, label: 2, bag_size: 176\n",
      "batch 159, loss: 2.4510, instance_loss: 0.1928, weighted_loss: 1.3219, label: 2, bag_size: 214\n",
      "batch 179, loss: 1.8250, instance_loss: 0.5955, weighted_loss: 1.2102, label: 2, bag_size: 120\n",
      "batch 199, loss: 0.9190, instance_loss: 0.8057, weighted_loss: 0.8623, label: 1, bag_size: 163\n",
      "batch 219, loss: 1.5961, instance_loss: 1.2288, weighted_loss: 1.4125, label: 1, bag_size: 125\n",
      "batch 239, loss: 1.2460, instance_loss: 0.1717, weighted_loss: 0.7088, label: 0, bag_size: 162\n",
      "batch 259, loss: 0.7314, instance_loss: 0.2143, weighted_loss: 0.4729, label: 0, bag_size: 139\n",
      "batch 279, loss: 0.7992, instance_loss: 0.8463, weighted_loss: 0.8227, label: 1, bag_size: 150\n",
      "batch 299, loss: 0.8009, instance_loss: 0.8033, weighted_loss: 0.8021, label: 1, bag_size: 151\n",
      "batch 319, loss: 1.2582, instance_loss: 0.2133, weighted_loss: 0.7358, label: 0, bag_size: 126\n",
      "batch 339, loss: 0.9732, instance_loss: 0.8637, weighted_loss: 0.9184, label: 1, bag_size: 219\n",
      "batch 359, loss: 1.2005, instance_loss: 0.3670, weighted_loss: 0.7837, label: 0, bag_size: 121\n",
      "batch 379, loss: 0.6333, instance_loss: 0.2560, weighted_loss: 0.4447, label: 0, bag_size: 176\n",
      "batch 399, loss: 0.9878, instance_loss: 0.1406, weighted_loss: 0.5642, label: 2, bag_size: 146\n",
      "batch 419, loss: 0.9546, instance_loss: 0.1052, weighted_loss: 0.5299, label: 0, bag_size: 194\n",
      "batch 439, loss: 2.2758, instance_loss: 0.1386, weighted_loss: 1.2072, label: 4, bag_size: 88\n",
      "batch 459, loss: 0.8090, instance_loss: 0.2573, weighted_loss: 0.5332, label: 0, bag_size: 66\n",
      "batch 479, loss: 1.0058, instance_loss: 0.8926, weighted_loss: 0.9492, label: 1, bag_size: 162\n",
      "batch 499, loss: 1.1016, instance_loss: 0.8695, weighted_loss: 0.9856, label: 1, bag_size: 97\n",
      "batch 519, loss: 2.4114, instance_loss: 0.1943, weighted_loss: 1.3029, label: 4, bag_size: 315\n",
      "batch 539, loss: 1.5075, instance_loss: 0.1341, weighted_loss: 0.8208, label: 4, bag_size: 94\n",
      "batch 559, loss: 1.2322, instance_loss: 1.0887, weighted_loss: 1.1605, label: 1, bag_size: 193\n",
      "batch 579, loss: 1.7250, instance_loss: 0.1200, weighted_loss: 0.9225, label: 3, bag_size: 60\n",
      "batch 599, loss: 0.9834, instance_loss: 0.0622, weighted_loss: 0.5228, label: 4, bag_size: 102\n",
      "Epoch: 1, train_loss: 1.5258, train_clustering_loss:  0.4058, train_error: 0.6579\n",
      "class 0: acc 0.4634146341463415, correct 76/164\n",
      "class 1: acc 0.47368421052631576, correct 81/171\n",
      "class 2: acc 0.08333333333333333, correct 6/72\n",
      "class 3: acc 0.04918032786885246, correct 3/61\n",
      "class 4: acc 0.18571428571428572, correct 13/70\n",
      "class 5: acc 0.4142857142857143, correct 29/70\n",
      "\n",
      "Val Set, val_loss: 1.5845, val_error: 0.6382\n",
      "class 0: acc 0.3953488372093023, correct 17/43\n",
      "class 1: acc 0.65, correct 26/40\n",
      "class 2: acc 0.0, correct 0/25\n",
      "class 3: acc 0.0, correct 0/12\n",
      "class 4: acc 0.8, correct 12/15\n",
      "class 5: acc 0.0, correct 0/17\n",
      "\n",
      "\n",
      "batch 19, loss: 2.0488, instance_loss: 0.0476, weighted_loss: 1.0482, label: 0, bag_size: 192\n",
      "batch 39, loss: 0.4779, instance_loss: 0.1693, weighted_loss: 0.3236, label: 0, bag_size: 193\n",
      "batch 59, loss: 1.7784, instance_loss: 0.7122, weighted_loss: 1.2453, label: 1, bag_size: 107\n",
      "batch 79, loss: 1.2255, instance_loss: 0.1246, weighted_loss: 0.6751, label: 5, bag_size: 197\n",
      "batch 99, loss: 1.7009, instance_loss: 0.0577, weighted_loss: 0.8793, label: 0, bag_size: 115\n",
      "batch 119, loss: 1.4064, instance_loss: 0.1649, weighted_loss: 0.7856, label: 0, bag_size: 167\n",
      "batch 139, loss: 0.6952, instance_loss: 0.2398, weighted_loss: 0.4675, label: 0, bag_size: 189\n",
      "batch 159, loss: 1.3285, instance_loss: 0.2272, weighted_loss: 0.7778, label: 0, bag_size: 118\n",
      "batch 179, loss: 1.5910, instance_loss: 1.1563, weighted_loss: 1.3737, label: 1, bag_size: 173\n",
      "batch 199, loss: 1.2088, instance_loss: 0.1061, weighted_loss: 0.6575, label: 4, bag_size: 165\n",
      "batch 219, loss: 0.5041, instance_loss: 0.1074, weighted_loss: 0.3057, label: 0, bag_size: 123\n",
      "batch 239, loss: 1.7431, instance_loss: 1.2284, weighted_loss: 1.4858, label: 1, bag_size: 251\n",
      "batch 259, loss: 1.3246, instance_loss: 1.0160, weighted_loss: 1.1703, label: 1, bag_size: 135\n",
      "batch 279, loss: 0.7662, instance_loss: 0.8407, weighted_loss: 0.8035, label: 1, bag_size: 206\n",
      "batch 299, loss: 0.8164, instance_loss: 0.2042, weighted_loss: 0.5103, label: 0, bag_size: 326\n",
      "batch 319, loss: 0.7223, instance_loss: 0.8589, weighted_loss: 0.7906, label: 1, bag_size: 151\n",
      "batch 339, loss: 0.5703, instance_loss: 0.7757, weighted_loss: 0.6730, label: 1, bag_size: 163\n",
      "batch 359, loss: 0.7740, instance_loss: 0.7979, weighted_loss: 0.7860, label: 1, bag_size: 116\n",
      "batch 379, loss: 2.8000, instance_loss: 0.3124, weighted_loss: 1.5562, label: 0, bag_size: 121\n",
      "batch 399, loss: 1.4026, instance_loss: 0.1548, weighted_loss: 0.7787, label: 0, bag_size: 119\n",
      "batch 419, loss: 2.1130, instance_loss: 0.0610, weighted_loss: 1.0870, label: 5, bag_size: 255\n",
      "batch 439, loss: 1.5946, instance_loss: 0.1542, weighted_loss: 0.8744, label: 0, bag_size: 123\n",
      "batch 459, loss: 0.5468, instance_loss: 0.2193, weighted_loss: 0.3830, label: 0, bag_size: 103\n",
      "batch 479, loss: 3.1957, instance_loss: 0.2225, weighted_loss: 1.7091, label: 3, bag_size: 158\n",
      "batch 499, loss: 1.4500, instance_loss: 0.1415, weighted_loss: 0.7957, label: 4, bag_size: 185\n",
      "batch 519, loss: 1.6746, instance_loss: 1.0852, weighted_loss: 1.3799, label: 1, bag_size: 237\n",
      "batch 539, loss: 2.0709, instance_loss: 0.1162, weighted_loss: 1.0936, label: 2, bag_size: 205\n",
      "batch 559, loss: 0.8241, instance_loss: 0.3582, weighted_loss: 0.5912, label: 0, bag_size: 142\n",
      "batch 579, loss: 1.3726, instance_loss: 0.1288, weighted_loss: 0.7507, label: 5, bag_size: 151\n",
      "batch 599, loss: 0.9426, instance_loss: 0.8477, weighted_loss: 0.8952, label: 1, bag_size: 117\n",
      "Epoch: 2, train_loss: 1.4340, train_clustering_loss:  0.3944, train_error: 0.5724\n",
      "class 0: acc 0.6585365853658537, correct 108/164\n",
      "class 1: acc 0.52046783625731, correct 89/171\n",
      "class 2: acc 0.027777777777777776, correct 2/72\n",
      "class 3: acc 0.18032786885245902, correct 11/61\n",
      "class 4: acc 0.2714285714285714, correct 19/70\n",
      "class 5: acc 0.44285714285714284, correct 31/70\n",
      "\n",
      "Val Set, val_loss: 1.4032, val_error: 0.5789\n",
      "class 0: acc 0.6744186046511628, correct 29/43\n",
      "class 1: acc 0.55, correct 22/40\n",
      "class 2: acc 0.0, correct 0/25\n",
      "class 3: acc 0.5, correct 6/12\n",
      "class 4: acc 0.06666666666666667, correct 1/15\n",
      "class 5: acc 0.35294117647058826, correct 6/17\n",
      "\n",
      "\n",
      "batch 19, loss: 1.7294, instance_loss: 0.7450, weighted_loss: 1.2372, label: 1, bag_size: 123\n",
      "batch 39, loss: 2.1289, instance_loss: 0.1425, weighted_loss: 1.1357, label: 4, bag_size: 106\n",
      "batch 59, loss: 0.9251, instance_loss: 0.3958, weighted_loss: 0.6604, label: 0, bag_size: 202\n",
      "batch 79, loss: 1.6938, instance_loss: 0.2500, weighted_loss: 0.9719, label: 2, bag_size: 140\n",
      "batch 99, loss: 0.7822, instance_loss: 0.7421, weighted_loss: 0.7621, label: 1, bag_size: 206\n",
      "batch 119, loss: 1.4445, instance_loss: 0.2130, weighted_loss: 0.8288, label: 4, bag_size: 167\n",
      "batch 139, loss: 0.6708, instance_loss: 0.2437, weighted_loss: 0.4572, label: 0, bag_size: 184\n",
      "batch 159, loss: 1.1838, instance_loss: 0.2134, weighted_loss: 0.6986, label: 2, bag_size: 153\n",
      "batch 179, loss: 0.5931, instance_loss: 0.0570, weighted_loss: 0.3250, label: 5, bag_size: 91\n",
      "batch 199, loss: 1.9823, instance_loss: 1.1192, weighted_loss: 1.5508, label: 1, bag_size: 187\n",
      "batch 219, loss: 0.1702, instance_loss: 0.0840, weighted_loss: 0.1271, label: 0, bag_size: 32\n",
      "batch 239, loss: 0.2875, instance_loss: 0.2303, weighted_loss: 0.2589, label: 0, bag_size: 202\n",
      "batch 259, loss: 1.1107, instance_loss: 0.1668, weighted_loss: 0.6388, label: 3, bag_size: 169\n",
      "batch 279, loss: 0.9224, instance_loss: 1.1648, weighted_loss: 1.0436, label: 1, bag_size: 141\n",
      "batch 299, loss: 1.0895, instance_loss: 0.1085, weighted_loss: 0.5990, label: 4, bag_size: 181\n",
      "batch 319, loss: 1.8522, instance_loss: 0.0891, weighted_loss: 0.9707, label: 4, bag_size: 177\n",
      "batch 339, loss: 1.5061, instance_loss: 0.1420, weighted_loss: 0.8240, label: 0, bag_size: 154\n",
      "batch 359, loss: 1.9800, instance_loss: 0.2538, weighted_loss: 1.1169, label: 2, bag_size: 280\n",
      "batch 379, loss: 1.3989, instance_loss: 0.2833, weighted_loss: 0.8411, label: 0, bag_size: 100\n",
      "batch 399, loss: 0.6210, instance_loss: 0.1541, weighted_loss: 0.3875, label: 0, bag_size: 63\n",
      "batch 419, loss: 0.5965, instance_loss: 0.0750, weighted_loss: 0.3358, label: 0, bag_size: 214\n",
      "batch 439, loss: 0.3422, instance_loss: 0.2981, weighted_loss: 0.3201, label: 0, bag_size: 161\n",
      "batch 459, loss: 0.7130, instance_loss: 0.2436, weighted_loss: 0.4783, label: 0, bag_size: 195\n",
      "batch 479, loss: 2.1225, instance_loss: 0.1271, weighted_loss: 1.1248, label: 0, bag_size: 192\n",
      "batch 499, loss: 1.2409, instance_loss: 0.9565, weighted_loss: 1.0987, label: 1, bag_size: 219\n",
      "batch 519, loss: 2.2703, instance_loss: 0.3608, weighted_loss: 1.3155, label: 0, bag_size: 189\n",
      "batch 539, loss: 0.7949, instance_loss: 0.7848, weighted_loss: 0.7898, label: 1, bag_size: 189\n",
      "batch 559, loss: 2.4186, instance_loss: 0.2341, weighted_loss: 1.3264, label: 4, bag_size: 315\n",
      "batch 579, loss: 0.8728, instance_loss: 0.1544, weighted_loss: 0.5136, label: 0, bag_size: 119\n",
      "batch 599, loss: 1.8403, instance_loss: 0.2635, weighted_loss: 1.0519, label: 2, bag_size: 147\n",
      "Epoch: 3, train_loss: 1.3093, train_clustering_loss:  0.3764, train_error: 0.5164\n",
      "class 0: acc 0.7073170731707317, correct 116/164\n",
      "class 1: acc 0.5964912280701754, correct 102/171\n",
      "class 2: acc 0.06944444444444445, correct 5/72\n",
      "class 3: acc 0.26229508196721313, correct 16/61\n",
      "class 4: acc 0.21428571428571427, correct 15/70\n",
      "class 5: acc 0.5714285714285714, correct 40/70\n",
      "\n",
      "Val Set, val_loss: 1.3116, val_error: 0.4934\n",
      "class 0: acc 0.9534883720930233, correct 41/43\n",
      "class 1: acc 0.6, correct 24/40\n",
      "class 2: acc 0.0, correct 0/25\n",
      "class 3: acc 0.0, correct 0/12\n",
      "class 4: acc 0.0, correct 0/15\n",
      "class 5: acc 0.7058823529411765, correct 12/17\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3783, instance_loss: 0.1832, weighted_loss: 0.2807, label: 0, bag_size: 146\n",
      "batch 39, loss: 1.6450, instance_loss: 0.8388, weighted_loss: 1.2419, label: 1, bag_size: 110\n",
      "batch 59, loss: 1.7830, instance_loss: 0.2810, weighted_loss: 1.0320, label: 4, bag_size: 102\n",
      "batch 79, loss: 1.5769, instance_loss: 0.1825, weighted_loss: 0.8797, label: 3, bag_size: 139\n",
      "batch 99, loss: 0.1439, instance_loss: 0.1114, weighted_loss: 0.1276, label: 0, bag_size: 326\n",
      "batch 119, loss: 0.1084, instance_loss: 0.0731, weighted_loss: 0.0908, label: 0, bag_size: 174\n",
      "batch 139, loss: 1.1074, instance_loss: 0.8276, weighted_loss: 0.9675, label: 1, bag_size: 206\n",
      "batch 159, loss: 0.7045, instance_loss: 0.5920, weighted_loss: 0.6482, label: 1, bag_size: 212\n",
      "batch 179, loss: 0.6601, instance_loss: 0.3115, weighted_loss: 0.4858, label: 0, bag_size: 119\n",
      "batch 199, loss: 1.6448, instance_loss: 0.1966, weighted_loss: 0.9207, label: 3, bag_size: 36\n",
      "batch 219, loss: 1.6654, instance_loss: 0.2695, weighted_loss: 0.9674, label: 2, bag_size: 175\n",
      "batch 239, loss: 2.0882, instance_loss: 0.0528, weighted_loss: 1.0705, label: 4, bag_size: 40\n",
      "batch 259, loss: 0.8792, instance_loss: 1.0847, weighted_loss: 0.9820, label: 1, bag_size: 148\n",
      "batch 279, loss: 0.6471, instance_loss: 0.8228, weighted_loss: 0.7349, label: 1, bag_size: 189\n",
      "batch 299, loss: 0.4615, instance_loss: 0.8011, weighted_loss: 0.6313, label: 1, bag_size: 140\n",
      "batch 319, loss: 1.3413, instance_loss: 0.2112, weighted_loss: 0.7763, label: 0, bag_size: 166\n",
      "batch 339, loss: 0.5927, instance_loss: 0.2644, weighted_loss: 0.4286, label: 0, bag_size: 156\n",
      "batch 359, loss: 1.3802, instance_loss: 1.0281, weighted_loss: 1.2042, label: 1, bag_size: 201\n",
      "batch 379, loss: 0.1547, instance_loss: 0.0674, weighted_loss: 0.1111, label: 0, bag_size: 103\n",
      "batch 399, loss: 0.2424, instance_loss: 0.2196, weighted_loss: 0.2310, label: 0, bag_size: 217\n",
      "batch 419, loss: 0.2934, instance_loss: 0.2247, weighted_loss: 0.2590, label: 0, bag_size: 202\n",
      "batch 439, loss: 1.1800, instance_loss: 0.0954, weighted_loss: 0.6377, label: 4, bag_size: 41\n",
      "batch 459, loss: 0.8294, instance_loss: 0.8258, weighted_loss: 0.8276, label: 1, bag_size: 114\n",
      "batch 479, loss: 1.8999, instance_loss: 0.0697, weighted_loss: 0.9848, label: 5, bag_size: 134\n",
      "batch 499, loss: 1.3907, instance_loss: 0.1437, weighted_loss: 0.7672, label: 4, bag_size: 109\n",
      "batch 519, loss: 1.0270, instance_loss: 0.8749, weighted_loss: 0.9510, label: 1, bag_size: 206\n",
      "batch 539, loss: 0.7508, instance_loss: 0.8245, weighted_loss: 0.7876, label: 1, bag_size: 134\n",
      "batch 559, loss: 0.3341, instance_loss: 0.2046, weighted_loss: 0.2694, label: 0, bag_size: 347\n",
      "batch 579, loss: 1.7609, instance_loss: 0.2348, weighted_loss: 0.9979, label: 2, bag_size: 85\n",
      "batch 599, loss: 0.5546, instance_loss: 1.1822, weighted_loss: 0.8684, label: 1, bag_size: 250\n",
      "Epoch: 4, train_loss: 1.2371, train_clustering_loss:  0.3678, train_error: 0.5000\n",
      "class 0: acc 0.7317073170731707, correct 120/164\n",
      "class 1: acc 0.5964912280701754, correct 102/171\n",
      "class 2: acc 0.09722222222222222, correct 7/72\n",
      "class 3: acc 0.29508196721311475, correct 18/61\n",
      "class 4: acc 0.22857142857142856, correct 16/70\n",
      "class 5: acc 0.5857142857142857, correct 41/70\n",
      "\n",
      "Val Set, val_loss: 1.2243, val_error: 0.4868\n",
      "class 0: acc 0.9069767441860465, correct 39/43\n",
      "class 1: acc 0.425, correct 17/40\n",
      "class 2: acc 0.44, correct 11/25\n",
      "class 3: acc 0.0, correct 0/12\n",
      "class 4: acc 0.0, correct 0/15\n",
      "class 5: acc 0.6470588235294118, correct 11/17\n",
      "\n",
      "\n",
      "batch 19, loss: 1.0438, instance_loss: 0.1079, weighted_loss: 0.5758, label: 4, bag_size: 53\n",
      "batch 39, loss: 2.1204, instance_loss: 0.2088, weighted_loss: 1.1646, label: 3, bag_size: 158\n",
      "batch 59, loss: 1.4586, instance_loss: 0.1869, weighted_loss: 0.8228, label: 2, bag_size: 197\n",
      "batch 79, loss: 2.4550, instance_loss: 0.1931, weighted_loss: 1.3240, label: 4, bag_size: 160\n",
      "batch 99, loss: 2.4590, instance_loss: 0.2384, weighted_loss: 1.3487, label: 2, bag_size: 129\n",
      "batch 119, loss: 0.5584, instance_loss: 0.6229, weighted_loss: 0.5906, label: 1, bag_size: 156\n",
      "batch 139, loss: 1.7404, instance_loss: 0.2141, weighted_loss: 0.9772, label: 4, bag_size: 116\n",
      "batch 159, loss: 0.6938, instance_loss: 1.0886, weighted_loss: 0.8912, label: 1, bag_size: 205\n",
      "batch 179, loss: 1.0803, instance_loss: 0.9666, weighted_loss: 1.0235, label: 1, bag_size: 188\n",
      "batch 199, loss: 1.8198, instance_loss: 0.2760, weighted_loss: 1.0479, label: 2, bag_size: 195\n",
      "batch 219, loss: 1.3883, instance_loss: 0.1188, weighted_loss: 0.7536, label: 4, bag_size: 182\n",
      "batch 239, loss: 0.4111, instance_loss: 0.1417, weighted_loss: 0.2764, label: 0, bag_size: 200\n",
      "batch 259, loss: 0.2901, instance_loss: 0.1967, weighted_loss: 0.2434, label: 0, bag_size: 240\n",
      "batch 279, loss: 0.1228, instance_loss: 0.1062, weighted_loss: 0.1145, label: 0, bag_size: 89\n",
      "batch 299, loss: 0.6737, instance_loss: 0.6254, weighted_loss: 0.6496, label: 1, bag_size: 259\n",
      "batch 319, loss: 0.2539, instance_loss: 0.1627, weighted_loss: 0.2083, label: 0, bag_size: 269\n",
      "batch 339, loss: 2.4213, instance_loss: 1.2857, weighted_loss: 1.8535, label: 1, bag_size: 355\n",
      "batch 359, loss: 0.3675, instance_loss: 0.3736, weighted_loss: 0.3706, label: 0, bag_size: 166\n",
      "batch 379, loss: 0.3766, instance_loss: 0.2661, weighted_loss: 0.3213, label: 0, bag_size: 196\n",
      "batch 399, loss: 2.1922, instance_loss: 0.1565, weighted_loss: 1.1743, label: 4, bag_size: 170\n",
      "batch 419, loss: 1.3078, instance_loss: 0.1391, weighted_loss: 0.7235, label: 4, bag_size: 146\n",
      "batch 439, loss: 1.3880, instance_loss: 1.0949, weighted_loss: 1.2415, label: 1, bag_size: 212\n",
      "batch 459, loss: 1.1465, instance_loss: 0.0672, weighted_loss: 0.6069, label: 4, bag_size: 119\n",
      "batch 479, loss: 2.0555, instance_loss: 0.2618, weighted_loss: 1.1586, label: 2, bag_size: 184\n",
      "batch 499, loss: 1.4563, instance_loss: 0.0296, weighted_loss: 0.7430, label: 4, bag_size: 128\n",
      "batch 519, loss: 1.3631, instance_loss: 0.0704, weighted_loss: 0.7167, label: 3, bag_size: 177\n",
      "batch 539, loss: 1.7650, instance_loss: 0.0751, weighted_loss: 0.9200, label: 4, bag_size: 158\n",
      "batch 559, loss: 0.8363, instance_loss: 0.7527, weighted_loss: 0.7945, label: 1, bag_size: 91\n",
      "batch 579, loss: 1.0864, instance_loss: 1.2669, weighted_loss: 1.1767, label: 1, bag_size: 16\n",
      "batch 599, loss: 0.6651, instance_loss: 0.6810, weighted_loss: 0.6730, label: 1, bag_size: 132\n",
      "Epoch: 5, train_loss: 1.1885, train_clustering_loss:  0.3685, train_error: 0.4605\n",
      "class 0: acc 0.7621951219512195, correct 125/164\n",
      "class 1: acc 0.6257309941520468, correct 107/171\n",
      "class 2: acc 0.1388888888888889, correct 10/72\n",
      "class 3: acc 0.22950819672131148, correct 14/61\n",
      "class 4: acc 0.35714285714285715, correct 25/70\n",
      "class 5: acc 0.6714285714285714, correct 47/70\n",
      "\n",
      "Val Set, val_loss: 1.3469, val_error: 0.5658\n",
      "class 0: acc 0.5813953488372093, correct 25/43\n",
      "class 1: acc 0.575, correct 23/40\n",
      "class 2: acc 0.0, correct 0/25\n",
      "class 3: acc 0.5833333333333334, correct 7/12\n",
      "class 4: acc 0.4, correct 6/15\n",
      "class 5: acc 0.29411764705882354, correct 5/17\n",
      "\n",
      "\n",
      "batch 19, loss: 1.7568, instance_loss: 0.3660, weighted_loss: 1.0614, label: 0, bag_size: 200\n",
      "batch 39, loss: 1.4322, instance_loss: 0.1485, weighted_loss: 0.7904, label: 4, bag_size: 182\n",
      "batch 59, loss: 0.4624, instance_loss: 0.3706, weighted_loss: 0.4165, label: 0, bag_size: 201\n",
      "batch 79, loss: 0.9672, instance_loss: 0.7669, weighted_loss: 0.8671, label: 1, bag_size: 134\n",
      "batch 99, loss: 1.4844, instance_loss: 0.7796, weighted_loss: 1.1320, label: 1, bag_size: 151\n",
      "batch 119, loss: 1.3575, instance_loss: 0.1141, weighted_loss: 0.7358, label: 2, bag_size: 227\n",
      "batch 139, loss: 0.1743, instance_loss: 0.0835, weighted_loss: 0.1289, label: 0, bag_size: 101\n",
      "batch 159, loss: 1.2021, instance_loss: 0.1210, weighted_loss: 0.6615, label: 2, bag_size: 120\n",
      "batch 179, loss: 3.2716, instance_loss: 0.1238, weighted_loss: 1.6977, label: 3, bag_size: 43\n",
      "batch 199, loss: 0.1982, instance_loss: 0.1358, weighted_loss: 0.1670, label: 0, bag_size: 236\n",
      "batch 219, loss: 0.0967, instance_loss: 0.1296, weighted_loss: 0.1132, label: 0, bag_size: 176\n",
      "batch 239, loss: 2.2057, instance_loss: 0.0963, weighted_loss: 1.1510, label: 4, bag_size: 106\n",
      "batch 259, loss: 0.5931, instance_loss: 0.3274, weighted_loss: 0.4602, label: 0, bag_size: 240\n",
      "batch 279, loss: 2.7841, instance_loss: 0.2426, weighted_loss: 1.5134, label: 5, bag_size: 126\n",
      "batch 299, loss: 0.8034, instance_loss: 0.0851, weighted_loss: 0.4443, label: 4, bag_size: 181\n",
      "batch 319, loss: 1.4400, instance_loss: 0.0247, weighted_loss: 0.7324, label: 3, bag_size: 145\n",
      "batch 339, loss: 1.5884, instance_loss: 0.0435, weighted_loss: 0.8160, label: 5, bag_size: 55\n",
      "batch 359, loss: 1.1062, instance_loss: 0.2365, weighted_loss: 0.6713, label: 3, bag_size: 177\n",
      "batch 379, loss: 0.3788, instance_loss: 0.2002, weighted_loss: 0.2895, label: 0, bag_size: 152\n",
      "batch 399, loss: 1.6774, instance_loss: 0.0584, weighted_loss: 0.8679, label: 2, bag_size: 230\n",
      "batch 419, loss: 1.7273, instance_loss: 0.1350, weighted_loss: 0.9312, label: 2, bag_size: 176\n",
      "batch 439, loss: 0.2033, instance_loss: 0.0763, weighted_loss: 0.1398, label: 0, bag_size: 103\n",
      "batch 459, loss: 1.3529, instance_loss: 0.0605, weighted_loss: 0.7067, label: 5, bag_size: 221\n",
      "batch 479, loss: 0.2967, instance_loss: 0.6285, weighted_loss: 0.4626, label: 1, bag_size: 140\n",
      "batch 499, loss: 0.4533, instance_loss: 0.5786, weighted_loss: 0.5159, label: 1, bag_size: 109\n",
      "batch 519, loss: 0.7858, instance_loss: 0.2676, weighted_loss: 0.5267, label: 2, bag_size: 273\n",
      "batch 539, loss: 1.2828, instance_loss: 0.2692, weighted_loss: 0.7760, label: 2, bag_size: 153\n",
      "batch 559, loss: 1.3135, instance_loss: 0.0673, weighted_loss: 0.6904, label: 5, bag_size: 180\n",
      "batch 579, loss: 0.0426, instance_loss: 0.1012, weighted_loss: 0.0719, label: 0, bag_size: 146\n",
      "batch 599, loss: 0.3358, instance_loss: 0.4739, weighted_loss: 0.4048, label: 1, bag_size: 169\n",
      "Epoch: 6, train_loss: 1.1279, train_clustering_loss:  0.3622, train_error: 0.4688\n",
      "class 0: acc 0.774390243902439, correct 127/164\n",
      "class 1: acc 0.5847953216374269, correct 100/171\n",
      "class 2: acc 0.125, correct 9/72\n",
      "class 3: acc 0.2786885245901639, correct 17/61\n",
      "class 4: acc 0.4142857142857143, correct 29/70\n",
      "class 5: acc 0.5857142857142857, correct 41/70\n",
      "\n",
      "Val Set, val_loss: 1.2775, val_error: 0.5461\n",
      "class 0: acc 0.6744186046511628, correct 29/43\n",
      "class 1: acc 0.825, correct 33/40\n",
      "class 2: acc 0.0, correct 0/25\n",
      "class 3: acc 0.08333333333333333, correct 1/12\n",
      "class 4: acc 0.26666666666666666, correct 4/15\n",
      "class 5: acc 0.11764705882352941, correct 2/17\n",
      "\n",
      "\n",
      "batch 19, loss: 1.8378, instance_loss: 0.1662, weighted_loss: 1.0020, label: 4, bag_size: 180\n",
      "batch 39, loss: 1.4636, instance_loss: 0.1537, weighted_loss: 0.8087, label: 0, bag_size: 99\n",
      "batch 59, loss: 0.6398, instance_loss: 0.7643, weighted_loss: 0.7020, label: 1, bag_size: 235\n",
      "batch 79, loss: 1.4957, instance_loss: 0.1230, weighted_loss: 0.8094, label: 4, bag_size: 106\n",
      "batch 99, loss: 0.1148, instance_loss: 0.1765, weighted_loss: 0.1457, label: 0, bag_size: 111\n",
      "batch 119, loss: 1.3081, instance_loss: 0.1269, weighted_loss: 0.7175, label: 3, bag_size: 137\n",
      "batch 139, loss: 1.7328, instance_loss: 1.1298, weighted_loss: 1.4313, label: 1, bag_size: 210\n",
      "batch 159, loss: 1.3054, instance_loss: 0.1123, weighted_loss: 0.7088, label: 5, bag_size: 234\n",
      "batch 179, loss: 1.6915, instance_loss: 0.2154, weighted_loss: 0.9534, label: 2, bag_size: 109\n",
      "batch 199, loss: 0.3598, instance_loss: 0.7697, weighted_loss: 0.5647, label: 1, bag_size: 275\n",
      "batch 219, loss: 0.2352, instance_loss: 0.2296, weighted_loss: 0.2324, label: 0, bag_size: 200\n",
      "batch 239, loss: 5.0649, instance_loss: 0.3139, weighted_loss: 2.6894, label: 3, bag_size: 114\n",
      "batch 259, loss: 1.0840, instance_loss: 0.4903, weighted_loss: 0.7872, label: 2, bag_size: 128\n",
      "batch 279, loss: 3.0362, instance_loss: 0.2208, weighted_loss: 1.6285, label: 5, bag_size: 126\n",
      "batch 299, loss: 0.2952, instance_loss: 0.2071, weighted_loss: 0.2512, label: 0, bag_size: 100\n",
      "batch 319, loss: 0.3942, instance_loss: 0.6557, weighted_loss: 0.5249, label: 1, bag_size: 233\n",
      "batch 339, loss: 1.5284, instance_loss: 0.1324, weighted_loss: 0.8304, label: 2, bag_size: 176\n",
      "batch 359, loss: 0.6753, instance_loss: 0.6328, weighted_loss: 0.6540, label: 1, bag_size: 162\n",
      "batch 379, loss: 1.5672, instance_loss: 0.2775, weighted_loss: 0.9223, label: 4, bag_size: 167\n",
      "batch 399, loss: 0.9021, instance_loss: 0.0100, weighted_loss: 0.4560, label: 4, bag_size: 114\n",
      "batch 419, loss: 0.6372, instance_loss: 0.0203, weighted_loss: 0.3288, label: 4, bag_size: 168\n",
      "batch 439, loss: 1.1076, instance_loss: 1.4604, weighted_loss: 1.2840, label: 1, bag_size: 117\n",
      "batch 459, loss: 1.0037, instance_loss: 0.0676, weighted_loss: 0.5357, label: 4, bag_size: 99\n",
      "batch 479, loss: 2.3893, instance_loss: 0.2528, weighted_loss: 1.3210, label: 0, bag_size: 196\n",
      "batch 499, loss: 1.4887, instance_loss: 0.3698, weighted_loss: 0.9292, label: 0, bag_size: 169\n",
      "batch 519, loss: 1.9257, instance_loss: 0.1891, weighted_loss: 1.0574, label: 2, bag_size: 133\n",
      "batch 539, loss: 2.3948, instance_loss: 0.0309, weighted_loss: 1.2128, label: 4, bag_size: 185\n",
      "batch 559, loss: 1.8776, instance_loss: 0.1121, weighted_loss: 0.9948, label: 3, bag_size: 150\n",
      "batch 579, loss: 1.4657, instance_loss: 1.1891, weighted_loss: 1.3274, label: 1, bag_size: 237\n",
      "batch 599, loss: 1.9246, instance_loss: 0.0860, weighted_loss: 1.0053, label: 3, bag_size: 141\n",
      "Epoch: 7, train_loss: 1.0859, train_clustering_loss:  0.3626, train_error: 0.4260\n",
      "class 0: acc 0.823170731707317, correct 135/164\n",
      "class 1: acc 0.631578947368421, correct 108/171\n",
      "class 2: acc 0.1527777777777778, correct 11/72\n",
      "class 3: acc 0.2786885245901639, correct 17/61\n",
      "class 4: acc 0.4142857142857143, correct 29/70\n",
      "class 5: acc 0.7, correct 49/70\n",
      "\n",
      "Val Set, val_loss: 1.0926, val_error: 0.4474\n",
      "class 0: acc 0.8604651162790697, correct 37/43\n",
      "class 1: acc 0.65, correct 26/40\n",
      "class 2: acc 0.28, correct 7/25\n",
      "class 3: acc 0.16666666666666666, correct 2/12\n",
      "class 4: acc 0.26666666666666666, correct 4/15\n",
      "class 5: acc 0.47058823529411764, correct 8/17\n",
      "\n",
      "\n",
      "batch 19, loss: 1.1420, instance_loss: 0.2579, weighted_loss: 0.7000, label: 2, bag_size: 197\n",
      "batch 39, loss: 1.0746, instance_loss: 0.2649, weighted_loss: 0.6697, label: 2, bag_size: 153\n",
      "batch 59, loss: 1.2216, instance_loss: 0.5221, weighted_loss: 0.8719, label: 2, bag_size: 212\n",
      "batch 79, loss: 1.0377, instance_loss: 0.8446, weighted_loss: 0.9411, label: 1, bag_size: 167\n",
      "batch 99, loss: 0.1430, instance_loss: 0.1585, weighted_loss: 0.1508, label: 0, bag_size: 174\n",
      "batch 119, loss: 0.1600, instance_loss: 0.8179, weighted_loss: 0.4890, label: 1, bag_size: 184\n",
      "batch 139, loss: 1.4492, instance_loss: 0.1056, weighted_loss: 0.7774, label: 4, bag_size: 231\n",
      "batch 159, loss: 1.8435, instance_loss: 0.0892, weighted_loss: 0.9664, label: 4, bag_size: 105\n",
      "batch 179, loss: 0.6613, instance_loss: 0.6255, weighted_loss: 0.6434, label: 1, bag_size: 150\n",
      "batch 199, loss: 0.8034, instance_loss: 0.3321, weighted_loss: 0.5677, label: 0, bag_size: 155\n",
      "batch 219, loss: 0.1021, instance_loss: 0.1489, weighted_loss: 0.1255, label: 0, bag_size: 208\n",
      "batch 239, loss: 0.1547, instance_loss: 0.1995, weighted_loss: 0.1771, label: 0, bag_size: 214\n",
      "batch 259, loss: 0.6614, instance_loss: 0.2417, weighted_loss: 0.4516, label: 0, bag_size: 154\n",
      "batch 279, loss: 0.1560, instance_loss: 0.1702, weighted_loss: 0.1631, label: 0, bag_size: 152\n",
      "batch 299, loss: 1.4918, instance_loss: 0.9602, weighted_loss: 1.2260, label: 1, bag_size: 135\n",
      "batch 319, loss: 0.5938, instance_loss: 0.1020, weighted_loss: 0.3479, label: 0, bag_size: 81\n",
      "batch 339, loss: 1.6445, instance_loss: 0.8681, weighted_loss: 1.2563, label: 1, bag_size: 230\n",
      "batch 359, loss: 0.2007, instance_loss: 0.1165, weighted_loss: 0.1586, label: 0, bag_size: 131\n",
      "batch 379, loss: 0.2063, instance_loss: 0.1338, weighted_loss: 0.1701, label: 0, bag_size: 119\n",
      "batch 399, loss: 1.0258, instance_loss: 0.1052, weighted_loss: 0.5655, label: 3, bag_size: 101\n",
      "batch 419, loss: 0.1922, instance_loss: 0.7582, weighted_loss: 0.4752, label: 1, bag_size: 251\n",
      "batch 439, loss: 0.2412, instance_loss: 0.4520, weighted_loss: 0.3466, label: 1, bag_size: 181\n",
      "batch 459, loss: 1.8663, instance_loss: 0.7101, weighted_loss: 1.2882, label: 1, bag_size: 134\n",
      "batch 479, loss: 0.1683, instance_loss: 0.2058, weighted_loss: 0.1871, label: 0, bag_size: 181\n",
      "batch 499, loss: 0.8563, instance_loss: 0.2014, weighted_loss: 0.5289, label: 0, bag_size: 151\n",
      "batch 519, loss: 0.8806, instance_loss: 0.0695, weighted_loss: 0.4751, label: 0, bag_size: 141\n",
      "batch 539, loss: 1.4247, instance_loss: 0.2608, weighted_loss: 0.8427, label: 2, bag_size: 214\n",
      "batch 559, loss: 0.4425, instance_loss: 0.6195, weighted_loss: 0.5310, label: 1, bag_size: 164\n",
      "batch 579, loss: 0.4947, instance_loss: 0.9066, weighted_loss: 0.7006, label: 1, bag_size: 243\n",
      "batch 599, loss: 1.2057, instance_loss: 0.0274, weighted_loss: 0.6165, label: 4, bag_size: 213\n",
      "Epoch: 8, train_loss: 1.0442, train_clustering_loss:  0.3469, train_error: 0.4276\n",
      "class 0: acc 0.7987804878048781, correct 131/164\n",
      "class 1: acc 0.6432748538011696, correct 110/171\n",
      "class 2: acc 0.2638888888888889, correct 19/72\n",
      "class 3: acc 0.21311475409836064, correct 13/61\n",
      "class 4: acc 0.37142857142857144, correct 26/70\n",
      "class 5: acc 0.7, correct 49/70\n",
      "\n",
      "Val Set, val_loss: 1.4245, val_error: 0.5921\n",
      "class 0: acc 0.5348837209302325, correct 23/43\n",
      "class 1: acc 0.25, correct 10/40\n",
      "class 2: acc 0.56, correct 14/25\n",
      "class 3: acc 0.0, correct 0/12\n",
      "class 4: acc 0.13333333333333333, correct 2/15\n",
      "class 5: acc 0.7647058823529411, correct 13/17\n",
      "\n",
      "\n",
      "batch 19, loss: 1.6062, instance_loss: 0.0379, weighted_loss: 0.8220, label: 4, bag_size: 266\n",
      "batch 39, loss: 0.6313, instance_loss: 0.7436, weighted_loss: 0.6875, label: 1, bag_size: 162\n",
      "batch 59, loss: 1.4566, instance_loss: 0.2894, weighted_loss: 0.8730, label: 2, bag_size: 290\n",
      "batch 79, loss: 2.1903, instance_loss: 0.3588, weighted_loss: 1.2745, label: 2, bag_size: 140\n",
      "batch 99, loss: 0.4710, instance_loss: 0.4847, weighted_loss: 0.4779, label: 1, bag_size: 123\n",
      "batch 119, loss: 1.2839, instance_loss: 0.0577, weighted_loss: 0.6708, label: 3, bag_size: 221\n",
      "batch 139, loss: 2.2079, instance_loss: 0.0769, weighted_loss: 1.1424, label: 3, bag_size: 90\n",
      "batch 159, loss: 1.1035, instance_loss: 1.6343, weighted_loss: 1.3689, label: 1, bag_size: 163\n",
      "batch 179, loss: 2.7426, instance_loss: 0.3377, weighted_loss: 1.5402, label: 0, bag_size: 121\n",
      "batch 199, loss: 0.6897, instance_loss: 0.7456, weighted_loss: 0.7177, label: 1, bag_size: 247\n",
      "batch 219, loss: 0.9610, instance_loss: 0.0414, weighted_loss: 0.5012, label: 4, bag_size: 28\n",
      "batch 239, loss: 0.5265, instance_loss: 0.1686, weighted_loss: 0.3475, label: 0, bag_size: 91\n",
      "batch 259, loss: 0.5372, instance_loss: 0.8124, weighted_loss: 0.6748, label: 1, bag_size: 280\n",
      "batch 279, loss: 0.2132, instance_loss: 0.6323, weighted_loss: 0.4227, label: 1, bag_size: 251\n",
      "batch 299, loss: 1.8938, instance_loss: 0.2963, weighted_loss: 1.0950, label: 2, bag_size: 189\n",
      "batch 319, loss: 1.8408, instance_loss: 0.2721, weighted_loss: 1.0565, label: 3, bag_size: 216\n",
      "batch 339, loss: 1.1585, instance_loss: 0.1758, weighted_loss: 0.6671, label: 2, bag_size: 218\n",
      "batch 359, loss: 0.4519, instance_loss: 0.0529, weighted_loss: 0.2524, label: 0, bag_size: 147\n",
      "batch 379, loss: 0.1642, instance_loss: 0.0081, weighted_loss: 0.0861, label: 5, bag_size: 79\n",
      "batch 399, loss: 0.1015, instance_loss: 0.1221, weighted_loss: 0.1118, label: 0, bag_size: 158\n",
      "batch 419, loss: 1.0896, instance_loss: 0.8083, weighted_loss: 0.9490, label: 1, bag_size: 135\n",
      "batch 439, loss: 1.2619, instance_loss: 0.0958, weighted_loss: 0.6789, label: 3, bag_size: 115\n",
      "batch 459, loss: 1.9699, instance_loss: 0.0970, weighted_loss: 1.0334, label: 5, bag_size: 198\n",
      "batch 479, loss: 0.5841, instance_loss: 0.6007, weighted_loss: 0.5924, label: 1, bag_size: 180\n",
      "batch 499, loss: 1.4098, instance_loss: 0.1045, weighted_loss: 0.7572, label: 3, bag_size: 168\n",
      "batch 519, loss: 0.4812, instance_loss: 0.2005, weighted_loss: 0.3408, label: 0, bag_size: 170\n",
      "batch 539, loss: 2.8567, instance_loss: 0.0075, weighted_loss: 1.4321, label: 4, bag_size: 139\n",
      "batch 559, loss: 0.2929, instance_loss: 0.5840, weighted_loss: 0.4384, label: 1, bag_size: 159\n",
      "batch 579, loss: 0.9857, instance_loss: 0.8992, weighted_loss: 0.9425, label: 1, bag_size: 204\n",
      "batch 599, loss: 1.6227, instance_loss: 0.9287, weighted_loss: 1.2757, label: 1, bag_size: 290\n",
      "Epoch: 9, train_loss: 1.0426, train_clustering_loss:  0.3548, train_error: 0.4211\n",
      "class 0: acc 0.8048780487804879, correct 132/164\n",
      "class 1: acc 0.6608187134502924, correct 113/171\n",
      "class 2: acc 0.1527777777777778, correct 11/72\n",
      "class 3: acc 0.32786885245901637, correct 20/61\n",
      "class 4: acc 0.4, correct 28/70\n",
      "class 5: acc 0.6857142857142857, correct 48/70\n",
      "\n",
      "Val Set, val_loss: 1.1662, val_error: 0.4868\n",
      "class 0: acc 0.7674418604651163, correct 33/43\n",
      "class 1: acc 0.5, correct 20/40\n",
      "class 2: acc 0.32, correct 8/25\n",
      "class 3: acc 0.08333333333333333, correct 1/12\n",
      "class 4: acc 0.13333333333333333, correct 2/15\n",
      "class 5: acc 0.8235294117647058, correct 14/17\n"
     ]
    }
   ],
   "source": [
    "from deep_learning.loaders.feature_dataset import FeatureDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from deep_learning.train.train import train\n",
    "from deep_learning.models.attention_core.clam import Clam\n",
    "\n",
    "df=pd.read_csv(\"F:/data/small.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "model=Clam(\n",
    "    feature_vector_length=2048,\n",
    "    dropout=0.1,\n",
    "    k_sample=8,\n",
    "    n_classes=6,\n",
    "    subtyping=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train(model,train_df,\"F:/extracted_features\",10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
