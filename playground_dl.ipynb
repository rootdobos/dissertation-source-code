{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([200, 2048, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from deep_learning.models.feature_extractors.resnet import Resnet50\n",
    "\n",
    "model= Resnet50()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "random_input = torch.randn(200, 3, 256, 256).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    features = model(random_input)\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.julia\\conda\\3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\.julia\\conda\\3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B1_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B1_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([200, 1280, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from deep_learning.models.feature_extractors.efficientnet import EfficientNetB1\n",
    "\n",
    "model= EfficientNetB1()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "random_input = torch.randn(200, 3, 256, 256).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    features = model(random_input)\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.julia\\conda\\3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\.julia\\conda\\3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from deep_learning.feature_extraction import extract_features\n",
    "from deep_learning.models.transforms.transforms import transform_resnet\n",
    "from deep_learning.models.feature_extractors.resnet import Resnet50\n",
    "from deep_learning.loaders.image_batch_loader import load_batch_from_dir\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=Resnet50().to(device)\n",
    "\n",
    "features=extract_features(model,transform_resnet,\"F:/test/0005f7aaab2800f6170c399693a96917/tiles\")\n",
    "torch.save(features,\"F:/test/0005f7aaab2800f6170c399693a96917.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'features/img1.npy'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame (assuming it’s loaded from a CSV)\n",
    "df = pd.DataFrame({\n",
    "    \"feature_path\": [\"features/img1.npy\", \"features/img2.npy\", \"features/img3.npy\"],\n",
    "    \"label\": [0, 1, 0]\n",
    "})\n",
    "df[\"feature_path\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 760\n",
      "Test size: 191\n",
      "Train batches: 760\n",
      "Batch X shape: torch.Size([193, 2048])\n",
      "Batch Y shape: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "from deep_learning.loaders.feature_dataset import FeatureDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "df=pd.read_csv(\"F:/data/small.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Test size:\", len(test_df))\n",
    "\n",
    "dataset=FeatureDataset(train_df,\"F:/extracted_features\")\n",
    "\n",
    "train_loader = DataLoader(dataset, shuffle=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "\n",
    "for batch in train_loader:\n",
    "    x_batch, y_batch = batch\n",
    "    x_batch=x_batch.squeeze()\n",
    "    print(\"Batch X shape:\", x_batch.shape)  # Expected: (32, D)\n",
    "    print(\"Batch Y shape:\", y_batch.squeeze().shape)  # Expected: (32,)\n",
    "    break  # Only one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Clam                                     [1, 5]                    5,130\n",
       "├─Sequential: 1-1                        [100, 5]                  --\n",
       "│    └─Linear: 2-1                       [100, 512]                1,049,088\n",
       "│    └─ReLU: 2-2                         [100, 512]                --\n",
       "│    └─Dropout: 2-3                      [100, 512]                --\n",
       "│    └─Attn_Net_Gated: 2-4               [100, 5]                  --\n",
       "│    │    └─Sequential: 3-1              [100, 256]                131,328\n",
       "│    │    └─Sequential: 3-2              [100, 256]                131,328\n",
       "│    │    └─Linear: 3-3                  [100, 5]                  1,285\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─Linear: 2-5                       [1]                       513\n",
       "│    └─Linear: 2-6                       [1]                       513\n",
       "│    └─Linear: 2-7                       [1]                       513\n",
       "│    └─Linear: 2-8                       [1]                       513\n",
       "│    └─Linear: 2-9                       [1]                       513\n",
       "==========================================================================================\n",
       "Total params: 1,320,724\n",
       "Trainable params: 1,320,724\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 131.31\n",
       "==========================================================================================\n",
       "Input size (MB): 0.82\n",
       "Forward/backward pass size (MB): 0.82\n",
       "Params size (MB): 5.26\n",
       "Estimated Total Size (MB): 6.90\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deep_learning.models.attention_core.clam import Clam\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=Clam(\n",
    "    feature_vector_length=2048,\n",
    "    dropout=0.1,\n",
    "    k_sample=8,\n",
    "    n_classes=5,\n",
    "    subtyping=True\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "summary(model, input_size=(100,2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0363, -0.0058,  0.0846, -0.0593,  0.0797]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    x_batch, y_batch = batch\n",
    "    x_batch=x_batch.squeeze()\n",
    "    x_batch.to(device)\n",
    "    logits, Y_prob, Y_hat, A_raw, results_dict=model(x_batch)\n",
    "    print(logits)\n",
    "    break  # Only one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "batch 19, loss: 1.3049, instance_loss: 0.1516, weighted_loss: 0.4976, label: 0, bag_size: 203\n",
      "batch 39, loss: 0.8241, instance_loss: 0.9548, weighted_loss: 0.9156, label: 1, bag_size: 150\n",
      "batch 59, loss: 1.2111, instance_loss: 0.2573, weighted_loss: 0.5434, label: 0, bag_size: 192\n",
      "batch 79, loss: 2.6438, instance_loss: 0.2657, weighted_loss: 0.9791, label: 2, bag_size: 212\n",
      "batch 99, loss: 1.4211, instance_loss: 0.1817, weighted_loss: 0.5535, label: 5, bag_size: 233\n",
      "batch 119, loss: 1.2094, instance_loss: 0.0477, weighted_loss: 0.3962, label: 5, bag_size: 199\n",
      "batch 139, loss: 2.5738, instance_loss: 0.1670, weighted_loss: 0.8891, label: 5, bag_size: 176\n",
      "batch 159, loss: 3.2032, instance_loss: 0.2288, weighted_loss: 1.1211, label: 4, bag_size: 160\n",
      "batch 179, loss: 1.8020, instance_loss: 1.1061, weighted_loss: 1.3149, label: 1, bag_size: 190\n",
      "batch 199, loss: 1.7292, instance_loss: 0.1540, weighted_loss: 0.6266, label: 2, bag_size: 138\n",
      "batch 219, loss: 1.4046, instance_loss: 1.1341, weighted_loss: 1.2153, label: 1, bag_size: 23\n",
      "batch 239, loss: 1.7875, instance_loss: 0.1292, weighted_loss: 0.6267, label: 0, bag_size: 65\n",
      "batch 259, loss: 1.8205, instance_loss: 0.1029, weighted_loss: 0.6182, label: 3, bag_size: 233\n",
      "batch 279, loss: 1.6574, instance_loss: 0.1443, weighted_loss: 0.5982, label: 2, bag_size: 128\n",
      "batch 299, loss: 1.3405, instance_loss: 0.2584, weighted_loss: 0.5830, label: 0, bag_size: 239\n",
      "batch 319, loss: 0.7804, instance_loss: 0.2004, weighted_loss: 0.3744, label: 0, bag_size: 94\n",
      "batch 339, loss: 2.5611, instance_loss: 0.1877, weighted_loss: 0.8997, label: 2, bag_size: 230\n",
      "batch 359, loss: 2.2198, instance_loss: 0.1623, weighted_loss: 0.7796, label: 5, bag_size: 176\n",
      "batch 379, loss: 1.5938, instance_loss: 0.1752, weighted_loss: 0.6008, label: 5, bag_size: 187\n",
      "batch 399, loss: 0.8051, instance_loss: 0.1680, weighted_loss: 0.3591, label: 0, bag_size: 330\n",
      "batch 419, loss: 1.6756, instance_loss: 0.1567, weighted_loss: 0.6123, label: 4, bag_size: 217\n",
      "batch 439, loss: 1.0883, instance_loss: 0.8812, weighted_loss: 0.9434, label: 1, bag_size: 210\n",
      "batch 459, loss: 1.4960, instance_loss: 0.3803, weighted_loss: 0.7150, label: 0, bag_size: 193\n",
      "batch 479, loss: 1.5016, instance_loss: 0.3099, weighted_loss: 0.6674, label: 0, bag_size: 169\n",
      "batch 499, loss: 1.9753, instance_loss: 0.1135, weighted_loss: 0.6721, label: 3, bag_size: 145\n",
      "batch 519, loss: 1.4744, instance_loss: 0.0724, weighted_loss: 0.4930, label: 3, bag_size: 43\n",
      "batch 539, loss: 1.3019, instance_loss: 0.9549, weighted_loss: 1.0590, label: 1, bag_size: 117\n",
      "batch 559, loss: 2.0336, instance_loss: 0.3468, weighted_loss: 0.8528, label: 2, bag_size: 225\n",
      "batch 579, loss: 1.8047, instance_loss: 0.0817, weighted_loss: 0.5986, label: 2, bag_size: 103\n",
      "batch 599, loss: 1.2772, instance_loss: 0.8979, weighted_loss: 1.0117, label: 1, bag_size: 147\n",
      "Epoch: 0, train_loss: 1.6498, train_clustering_loss:  255.5973, train_error: 0.7253\n",
      "class 0: acc 0.3902439024390244, correct 64/164\n",
      "class 1: acc 0.47953216374269003, correct 82/171\n",
      "class 2: acc 0.027777777777777776, correct 2/72\n",
      "class 3: acc 0.0, correct 0/61\n",
      "class 4: acc 0.08571428571428572, correct 6/70\n",
      "class 5: acc 0.18571428571428572, correct 13/70\n",
      "\n",
      "Val Set, val_loss: 1.6156, val_error: 0.7434\n",
      "class 0: acc 0.7906976744186046, correct 34/43\n",
      "class 1: acc 0.025, correct 1/40\n",
      "class 2: acc 0.16, correct 4/25\n",
      "class 3: acc 0.0, correct 0/12\n",
      "class 4: acc 0.0, correct 0/15\n",
      "class 5: acc 0.0, correct 0/17\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7629, instance_loss: 0.1782, weighted_loss: 0.3536, label: 0, bag_size: 218\n",
      "batch 39, loss: 2.2756, instance_loss: 0.0617, weighted_loss: 0.7259, label: 3, bag_size: 216\n",
      "batch 59, loss: 1.2888, instance_loss: 0.0323, weighted_loss: 0.4092, label: 4, bag_size: 116\n",
      "batch 79, loss: 2.0898, instance_loss: 0.0737, weighted_loss: 0.6785, label: 0, bag_size: 175\n",
      "batch 99, loss: 1.2590, instance_loss: 1.0095, weighted_loss: 1.0843, label: 1, bag_size: 107\n",
      "batch 119, loss: 1.8143, instance_loss: 0.6060, weighted_loss: 0.9685, label: 4, bag_size: 105\n",
      "batch 139, loss: 1.7867, instance_loss: 0.5998, weighted_loss: 0.9559, label: 0, bag_size: 118\n",
      "batch 159, loss: 1.2330, instance_loss: 0.1207, weighted_loss: 0.4544, label: 0, bag_size: 31\n",
      "batch 179, loss: 0.9736, instance_loss: 0.2172, weighted_loss: 0.4441, label: 0, bag_size: 156\n",
      "batch 199, loss: 0.9553, instance_loss: 0.2538, weighted_loss: 0.4642, label: 0, bag_size: 174\n",
      "batch 219, loss: 0.9796, instance_loss: 0.1936, weighted_loss: 0.4294, label: 0, bag_size: 219\n",
      "batch 239, loss: 0.8197, instance_loss: 0.2460, weighted_loss: 0.4181, label: 0, bag_size: 202\n",
      "batch 259, loss: 1.2065, instance_loss: 0.9483, weighted_loss: 1.0258, label: 1, bag_size: 221\n",
      "batch 279, loss: 1.9981, instance_loss: 0.2593, weighted_loss: 0.7810, label: 2, bag_size: 123\n",
      "batch 299, loss: 1.2469, instance_loss: 0.0619, weighted_loss: 0.4174, label: 4, bag_size: 157\n",
      "batch 319, loss: 1.8031, instance_loss: 0.1197, weighted_loss: 0.6247, label: 5, bag_size: 224\n",
      "batch 339, loss: 1.0395, instance_loss: 0.2150, weighted_loss: 0.4624, label: 0, bag_size: 228\n",
      "batch 359, loss: 0.9881, instance_loss: 0.9369, weighted_loss: 0.9523, label: 1, bag_size: 151\n",
      "batch 379, loss: 2.1763, instance_loss: 0.2086, weighted_loss: 0.7989, label: 2, bag_size: 184\n",
      "batch 399, loss: 0.5019, instance_loss: 0.1816, weighted_loss: 0.2777, label: 0, bag_size: 239\n",
      "batch 419, loss: 1.0075, instance_loss: 0.8706, weighted_loss: 0.9117, label: 1, bag_size: 132\n",
      "batch 439, loss: 2.5324, instance_loss: 0.1381, weighted_loss: 0.8564, label: 2, bag_size: 147\n",
      "batch 459, loss: 0.8875, instance_loss: 0.2513, weighted_loss: 0.4421, label: 0, bag_size: 167\n",
      "batch 479, loss: 0.6459, instance_loss: 0.1868, weighted_loss: 0.3245, label: 0, bag_size: 159\n",
      "batch 499, loss: 0.5732, instance_loss: 0.1841, weighted_loss: 0.3009, label: 0, bag_size: 202\n",
      "batch 519, loss: 1.4781, instance_loss: 0.0747, weighted_loss: 0.4957, label: 5, bag_size: 148\n",
      "batch 539, loss: 1.0203, instance_loss: 0.9671, weighted_loss: 0.9831, label: 1, bag_size: 229\n",
      "batch 559, loss: 1.4839, instance_loss: 1.0191, weighted_loss: 1.1585, label: 1, bag_size: 247\n",
      "batch 579, loss: 2.2989, instance_loss: 0.3275, weighted_loss: 0.9189, label: 4, bag_size: 187\n",
      "batch 599, loss: 0.8067, instance_loss: 0.6428, weighted_loss: 0.6920, label: 1, bag_size: 320\n",
      "Epoch: 1, train_loss: 1.5411, train_clustering_loss:  242.8721, train_error: 0.6349\n",
      "class 0: acc 0.5304878048780488, correct 87/164\n",
      "class 1: acc 0.47953216374269003, correct 82/171\n",
      "class 2: acc 0.05555555555555555, correct 4/72\n",
      "class 3: acc 0.11475409836065574, correct 7/61\n",
      "class 4: acc 0.24285714285714285, correct 17/70\n",
      "class 5: acc 0.35714285714285715, correct 25/70\n",
      "\n",
      "Val Set, val_loss: 1.5429, val_error: 0.7171\n",
      "class 0: acc 0.0, correct 0/43\n",
      "class 1: acc 0.8, correct 32/40\n",
      "class 2: acc 0.24, correct 6/25\n",
      "class 3: acc 0.0, correct 0/12\n",
      "class 4: acc 0.3333333333333333, correct 5/15\n",
      "class 5: acc 0.0, correct 0/17\n",
      "\n",
      "\n",
      "batch 19, loss: 1.2744, instance_loss: 0.4724, weighted_loss: 0.7130, label: 0, bag_size: 226\n",
      "batch 39, loss: 0.9158, instance_loss: 0.3878, weighted_loss: 0.5462, label: 0, bag_size: 475\n",
      "batch 59, loss: 0.9284, instance_loss: 0.7899, weighted_loss: 0.8315, label: 1, bag_size: 103\n",
      "batch 79, loss: 2.2854, instance_loss: 0.1312, weighted_loss: 0.7775, label: 2, bag_size: 290\n",
      "batch 99, loss: 2.1932, instance_loss: 0.1124, weighted_loss: 0.7366, label: 0, bag_size: 151\n",
      "batch 119, loss: 1.5856, instance_loss: 0.0771, weighted_loss: 0.5296, label: 4, bag_size: 9\n",
      "batch 139, loss: 1.7195, instance_loss: 0.1266, weighted_loss: 0.6044, label: 0, bag_size: 175\n",
      "batch 159, loss: 1.9288, instance_loss: 0.1523, weighted_loss: 0.6853, label: 2, bag_size: 103\n",
      "batch 179, loss: 2.1913, instance_loss: 0.0753, weighted_loss: 0.7101, label: 0, bag_size: 103\n",
      "batch 199, loss: 0.9316, instance_loss: 0.4491, weighted_loss: 0.5939, label: 0, bag_size: 240\n",
      "batch 219, loss: 2.4255, instance_loss: 1.3579, weighted_loss: 1.6782, label: 1, bag_size: 195\n",
      "batch 239, loss: 1.2824, instance_loss: 0.9681, weighted_loss: 1.0624, label: 1, bag_size: 162\n",
      "batch 259, loss: 0.6780, instance_loss: 0.6676, weighted_loss: 0.6707, label: 1, bag_size: 117\n",
      "batch 279, loss: 0.8323, instance_loss: 0.6866, weighted_loss: 0.7303, label: 1, bag_size: 159\n",
      "batch 299, loss: 0.8709, instance_loss: 0.3074, weighted_loss: 0.4764, label: 0, bag_size: 239\n",
      "batch 319, loss: 2.1658, instance_loss: 0.0630, weighted_loss: 0.6938, label: 5, bag_size: 167\n",
      "batch 339, loss: 2.0880, instance_loss: 0.1866, weighted_loss: 0.7570, label: 3, bag_size: 164\n",
      "batch 359, loss: 0.9277, instance_loss: 0.2709, weighted_loss: 0.4679, label: 0, bag_size: 94\n",
      "batch 379, loss: 0.9208, instance_loss: 0.9309, weighted_loss: 0.9278, label: 1, bag_size: 210\n",
      "batch 399, loss: 0.8706, instance_loss: 0.7414, weighted_loss: 0.7802, label: 1, bag_size: 148\n",
      "batch 419, loss: 2.3341, instance_loss: 0.1515, weighted_loss: 0.8063, label: 5, bag_size: 169\n",
      "batch 439, loss: 1.4998, instance_loss: 0.1480, weighted_loss: 0.5535, label: 5, bag_size: 195\n",
      "batch 459, loss: 0.9360, instance_loss: 0.0722, weighted_loss: 0.3314, label: 4, bag_size: 171\n",
      "batch 479, loss: 1.1269, instance_loss: 0.9716, weighted_loss: 1.0182, label: 1, bag_size: 127\n",
      "batch 499, loss: 1.5725, instance_loss: 1.2112, weighted_loss: 1.3196, label: 1, bag_size: 165\n",
      "batch 519, loss: 0.5777, instance_loss: 0.0448, weighted_loss: 0.2047, label: 5, bag_size: 211\n",
      "batch 539, loss: 0.2401, instance_loss: 0.0805, weighted_loss: 0.1284, label: 0, bag_size: 174\n",
      "batch 559, loss: 0.8778, instance_loss: 0.1133, weighted_loss: 0.3427, label: 0, bag_size: 154\n",
      "batch 579, loss: 1.5099, instance_loss: 0.1476, weighted_loss: 0.5563, label: 2, bag_size: 225\n",
      "batch 599, loss: 0.3136, instance_loss: 0.0219, weighted_loss: 0.1094, label: 5, bag_size: 145\n",
      "Epoch: 2, train_loss: 1.4298, train_clustering_loss:  235.2633, train_error: 0.6069\n",
      "class 0: acc 0.4268292682926829, correct 70/164\n",
      "class 1: acc 0.6023391812865497, correct 103/171\n",
      "class 2: acc 0.027777777777777776, correct 2/72\n",
      "class 3: acc 0.06557377049180328, correct 4/61\n",
      "class 4: acc 0.2857142857142857, correct 20/70\n",
      "class 5: acc 0.5714285714285714, correct 40/70\n",
      "\n",
      "Val Set, val_loss: 1.4028, val_error: 0.5263\n",
      "class 0: acc 0.7209302325581395, correct 31/43\n",
      "class 1: acc 0.625, correct 25/40\n",
      "class 2: acc 0.04, correct 1/25\n",
      "class 3: acc 0.25, correct 3/12\n",
      "class 4: acc 0.0, correct 0/15\n",
      "class 5: acc 0.7058823529411765, correct 12/17\n",
      "\n",
      "\n",
      "batch 19, loss: 1.0292, instance_loss: 0.9159, weighted_loss: 0.9499, label: 1, bag_size: 249\n",
      "batch 39, loss: 0.8003, instance_loss: 0.0333, weighted_loss: 0.2634, label: 5, bag_size: 237\n",
      "batch 59, loss: 2.1875, instance_loss: 0.0992, weighted_loss: 0.7257, label: 2, bag_size: 94\n",
      "batch 79, loss: 0.4258, instance_loss: 0.2123, weighted_loss: 0.2764, label: 0, bag_size: 172\n",
      "batch 99, loss: 0.2259, instance_loss: 0.1502, weighted_loss: 0.1729, label: 0, bag_size: 94\n",
      "batch 119, loss: 0.6311, instance_loss: 0.1773, weighted_loss: 0.3134, label: 0, bag_size: 119\n",
      "batch 139, loss: 1.4487, instance_loss: 0.0695, weighted_loss: 0.4833, label: 3, bag_size: 136\n",
      "batch 159, loss: 1.6577, instance_loss: 0.2951, weighted_loss: 0.7038, label: 2, bag_size: 109\n",
      "batch 179, loss: 1.2820, instance_loss: 0.1231, weighted_loss: 0.4708, label: 4, bag_size: 41\n",
      "batch 199, loss: 1.8823, instance_loss: 1.0949, weighted_loss: 1.3311, label: 1, bag_size: 151\n",
      "batch 219, loss: 1.1472, instance_loss: 0.1691, weighted_loss: 0.4625, label: 4, bag_size: 135\n",
      "batch 239, loss: 0.6035, instance_loss: 0.6543, weighted_loss: 0.6390, label: 1, bag_size: 250\n",
      "batch 259, loss: 1.3149, instance_loss: 0.0552, weighted_loss: 0.4332, label: 4, bag_size: 146\n",
      "batch 279, loss: 0.5820, instance_loss: 0.7893, weighted_loss: 0.7271, label: 1, bag_size: 140\n",
      "batch 299, loss: 1.3374, instance_loss: 0.2586, weighted_loss: 0.5823, label: 0, bag_size: 169\n",
      "batch 319, loss: 3.0149, instance_loss: 0.2641, weighted_loss: 1.0893, label: 5, bag_size: 181\n",
      "batch 339, loss: 1.4740, instance_loss: 0.1146, weighted_loss: 0.5224, label: 2, bag_size: 214\n",
      "batch 359, loss: 0.2936, instance_loss: 0.0799, weighted_loss: 0.1440, label: 0, bag_size: 63\n",
      "batch 379, loss: 0.9018, instance_loss: 0.7354, weighted_loss: 0.7853, label: 1, bag_size: 148\n",
      "batch 399, loss: 2.5676, instance_loss: 0.1488, weighted_loss: 0.8745, label: 3, bag_size: 158\n",
      "batch 419, loss: 1.2806, instance_loss: 0.3213, weighted_loss: 0.6091, label: 0, bag_size: 200\n",
      "batch 439, loss: 0.5925, instance_loss: 0.6277, weighted_loss: 0.6171, label: 1, bag_size: 150\n",
      "batch 459, loss: 2.3026, instance_loss: 0.3884, weighted_loss: 0.9627, label: 2, bag_size: 189\n",
      "batch 479, loss: 0.3592, instance_loss: 0.0391, weighted_loss: 0.1351, label: 5, bag_size: 152\n",
      "batch 499, loss: 1.3374, instance_loss: 1.2120, weighted_loss: 1.2496, label: 1, bag_size: 121\n",
      "batch 519, loss: 2.3708, instance_loss: 0.1296, weighted_loss: 0.8020, label: 3, bag_size: 216\n",
      "batch 539, loss: 1.4887, instance_loss: 0.3573, weighted_loss: 0.6967, label: 2, bag_size: 147\n",
      "batch 559, loss: 1.7294, instance_loss: 0.3289, weighted_loss: 0.7490, label: 2, bag_size: 155\n",
      "batch 579, loss: 1.9022, instance_loss: 0.3451, weighted_loss: 0.8122, label: 4, bag_size: 190\n",
      "batch 599, loss: 1.6670, instance_loss: 0.3452, weighted_loss: 0.7417, label: 2, bag_size: 225\n",
      "Epoch: 3, train_loss: 1.3176, train_clustering_loss:  224.5324, train_error: 0.5362\n",
      "class 0: acc 0.6707317073170732, correct 110/164\n",
      "class 1: acc 0.5614035087719298, correct 96/171\n",
      "class 2: acc 0.027777777777777776, correct 2/72\n",
      "class 3: acc 0.26229508196721313, correct 16/61\n",
      "class 4: acc 0.2571428571428571, correct 18/70\n",
      "class 5: acc 0.5714285714285714, correct 40/70\n",
      "\n",
      "Val Set, val_loss: 1.3225, val_error: 0.5263\n",
      "class 0: acc 0.5116279069767442, correct 22/43\n",
      "class 1: acc 0.85, correct 34/40\n",
      "class 2: acc 0.0, correct 0/25\n",
      "class 3: acc 0.16666666666666666, correct 2/12\n",
      "class 4: acc 0.13333333333333333, correct 2/15\n",
      "class 5: acc 0.7058823529411765, correct 12/17\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2873, instance_loss: 0.0478, weighted_loss: 0.1197, label: 5, bag_size: 150\n",
      "batch 39, loss: 0.4320, instance_loss: 0.2884, weighted_loss: 0.3315, label: 0, bag_size: 133\n",
      "batch 59, loss: 0.6802, instance_loss: 0.5894, weighted_loss: 0.6166, label: 1, bag_size: 181\n",
      "batch 79, loss: 1.3708, instance_loss: 0.0184, weighted_loss: 0.4241, label: 4, bag_size: 24\n",
      "batch 99, loss: 2.6871, instance_loss: 1.4603, weighted_loss: 1.8283, label: 1, bag_size: 122\n",
      "batch 119, loss: 0.3409, instance_loss: 0.1605, weighted_loss: 0.2146, label: 0, bag_size: 32\n",
      "batch 139, loss: 1.5907, instance_loss: 0.2298, weighted_loss: 0.6381, label: 2, bag_size: 135\n",
      "batch 159, loss: 0.9289, instance_loss: 0.0660, weighted_loss: 0.3248, label: 4, bag_size: 242\n",
      "batch 179, loss: 1.6992, instance_loss: 0.2469, weighted_loss: 0.6826, label: 0, bag_size: 169\n",
      "batch 199, loss: 1.8797, instance_loss: 0.0426, weighted_loss: 0.5937, label: 4, bag_size: 139\n",
      "batch 219, loss: 1.2959, instance_loss: 0.1737, weighted_loss: 0.5104, label: 3, bag_size: 36\n",
      "batch 239, loss: 0.2252, instance_loss: 0.2137, weighted_loss: 0.2172, label: 0, bag_size: 136\n",
      "batch 259, loss: 0.6706, instance_loss: 0.0485, weighted_loss: 0.2351, label: 5, bag_size: 74\n",
      "batch 279, loss: 2.8407, instance_loss: 0.1704, weighted_loss: 0.9715, label: 2, bag_size: 97\n",
      "batch 299, loss: 1.1248, instance_loss: 0.8639, weighted_loss: 0.9422, label: 1, bag_size: 187\n",
      "batch 319, loss: 2.3028, instance_loss: 0.2598, weighted_loss: 0.8727, label: 2, bag_size: 654\n",
      "batch 339, loss: 1.3058, instance_loss: 0.8147, weighted_loss: 0.9620, label: 1, bag_size: 159\n",
      "batch 359, loss: 1.9074, instance_loss: 0.0934, weighted_loss: 0.6376, label: 4, bag_size: 180\n",
      "batch 379, loss: 1.5766, instance_loss: 0.9126, weighted_loss: 1.1118, label: 1, bag_size: 107\n",
      "batch 399, loss: 2.4575, instance_loss: 0.0583, weighted_loss: 0.7780, label: 5, bag_size: 57\n",
      "batch 419, loss: 0.7098, instance_loss: 0.5556, weighted_loss: 0.6019, label: 1, bag_size: 52\n",
      "batch 439, loss: 2.7313, instance_loss: 0.4331, weighted_loss: 1.1226, label: 2, bag_size: 184\n",
      "batch 459, loss: 0.6306, instance_loss: 0.6337, weighted_loss: 0.6327, label: 1, bag_size: 175\n",
      "batch 479, loss: 1.3379, instance_loss: 1.1431, weighted_loss: 1.2016, label: 1, bag_size: 270\n",
      "batch 499, loss: 1.6148, instance_loss: 0.0666, weighted_loss: 0.5311, label: 3, bag_size: 216\n",
      "batch 519, loss: 1.3322, instance_loss: 0.0544, weighted_loss: 0.4377, label: 3, bag_size: 101\n",
      "batch 539, loss: 1.0438, instance_loss: 0.6814, weighted_loss: 0.7901, label: 1, bag_size: 137\n",
      "batch 559, loss: 0.9484, instance_loss: 1.0151, weighted_loss: 0.9951, label: 1, bag_size: 109\n",
      "batch 579, loss: 2.0425, instance_loss: 0.0272, weighted_loss: 0.6318, label: 4, bag_size: 185\n",
      "batch 599, loss: 0.5257, instance_loss: 0.1535, weighted_loss: 0.2651, label: 0, bag_size: 153\n",
      "Epoch: 4, train_loss: 1.2510, train_clustering_loss:  221.0224, train_error: 0.5049\n",
      "class 0: acc 0.6890243902439024, correct 113/164\n",
      "class 1: acc 0.6198830409356725, correct 106/171\n",
      "class 2: acc 0.05555555555555555, correct 4/72\n",
      "class 3: acc 0.26229508196721313, correct 16/61\n",
      "class 4: acc 0.24285714285714285, correct 17/70\n",
      "class 5: acc 0.6428571428571429, correct 45/70\n",
      "\n",
      "Val Set, val_loss: 1.2449, val_error: 0.5066\n",
      "class 0: acc 0.6046511627906976, correct 26/43\n",
      "class 1: acc 0.7, correct 28/40\n",
      "class 2: acc 0.08, correct 2/25\n",
      "class 3: acc 0.5833333333333334, correct 7/12\n",
      "class 4: acc 0.2, correct 3/15\n",
      "class 5: acc 0.5294117647058824, correct 9/17\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7556, instance_loss: 0.9318, weighted_loss: 0.8790, label: 1, bag_size: 189\n",
      "batch 39, loss: 2.2180, instance_loss: 0.2731, weighted_loss: 0.8566, label: 5, bag_size: 161\n",
      "batch 59, loss: 1.9996, instance_loss: 0.0795, weighted_loss: 0.6555, label: 5, bag_size: 173\n",
      "batch 79, loss: 1.7910, instance_loss: 0.1894, weighted_loss: 0.6699, label: 4, bag_size: 119\n",
      "batch 99, loss: 1.7654, instance_loss: 0.1794, weighted_loss: 0.6552, label: 4, bag_size: 106\n",
      "batch 119, loss: 2.5135, instance_loss: 0.2800, weighted_loss: 0.9501, label: 5, bag_size: 185\n",
      "batch 139, loss: 0.0783, instance_loss: 0.0104, weighted_loss: 0.0308, label: 5, bag_size: 199\n",
      "batch 159, loss: 1.1727, instance_loss: 0.1957, weighted_loss: 0.4888, label: 0, bag_size: 188\n",
      "batch 179, loss: 0.2385, instance_loss: 0.1831, weighted_loss: 0.1997, label: 0, bag_size: 196\n",
      "batch 199, loss: 0.2014, instance_loss: 0.2115, weighted_loss: 0.2085, label: 0, bag_size: 154\n",
      "batch 219, loss: 0.2071, instance_loss: 0.1269, weighted_loss: 0.1509, label: 0, bag_size: 115\n",
      "batch 239, loss: 1.1722, instance_loss: 0.9548, weighted_loss: 1.0200, label: 1, bag_size: 190\n",
      "batch 259, loss: 0.2759, instance_loss: 0.1379, weighted_loss: 0.1793, label: 0, bag_size: 277\n",
      "batch 279, loss: 0.5560, instance_loss: 0.6640, weighted_loss: 0.6316, label: 1, bag_size: 169\n",
      "batch 299, loss: 1.3471, instance_loss: 1.1385, weighted_loss: 1.2011, label: 1, bag_size: 150\n",
      "batch 319, loss: 2.0350, instance_loss: 0.1809, weighted_loss: 0.7371, label: 2, bag_size: 88\n",
      "batch 339, loss: 0.5175, instance_loss: 0.6511, weighted_loss: 0.6110, label: 1, bag_size: 163\n",
      "batch 359, loss: 1.4302, instance_loss: 0.2981, weighted_loss: 0.6378, label: 2, bag_size: 214\n",
      "batch 379, loss: 0.6568, instance_loss: 0.3781, weighted_loss: 0.4617, label: 0, bag_size: 165\n",
      "batch 399, loss: 1.5521, instance_loss: 0.1458, weighted_loss: 0.5677, label: 3, bag_size: 241\n",
      "batch 419, loss: 0.4913, instance_loss: 0.7242, weighted_loss: 0.6543, label: 1, bag_size: 291\n",
      "batch 439, loss: 0.4162, instance_loss: 0.6894, weighted_loss: 0.6074, label: 1, bag_size: 168\n",
      "batch 459, loss: 2.5766, instance_loss: 0.0683, weighted_loss: 0.8208, label: 5, bag_size: 257\n",
      "batch 479, loss: 1.9876, instance_loss: 0.0540, weighted_loss: 0.6341, label: 2, bag_size: 138\n",
      "batch 499, loss: 3.3300, instance_loss: 0.0526, weighted_loss: 1.0358, label: 0, bag_size: 150\n",
      "batch 519, loss: 0.7985, instance_loss: 0.0507, weighted_loss: 0.2750, label: 4, bag_size: 135\n",
      "batch 539, loss: 1.7180, instance_loss: 1.8221, weighted_loss: 1.7909, label: 1, bag_size: 16\n",
      "batch 559, loss: 0.4030, instance_loss: 0.2618, weighted_loss: 0.3042, label: 0, bag_size: 94\n",
      "batch 579, loss: 0.2208, instance_loss: 0.2414, weighted_loss: 0.2352, label: 0, bag_size: 198\n",
      "batch 599, loss: 0.1176, instance_loss: 0.1011, weighted_loss: 0.1061, label: 0, bag_size: 65\n",
      "Epoch: 5, train_loss: 1.1527, train_clustering_loss:  217.6711, train_error: 0.4671\n",
      "class 0: acc 0.7621951219512195, correct 125/164\n",
      "class 1: acc 0.6257309941520468, correct 107/171\n",
      "class 2: acc 0.16666666666666666, correct 12/72\n",
      "class 3: acc 0.21311475409836064, correct 13/61\n",
      "class 4: acc 0.32857142857142857, correct 23/70\n",
      "class 5: acc 0.6285714285714286, correct 44/70\n",
      "\n",
      "Val Set, val_loss: 1.1816, val_error: 0.5132\n",
      "class 0: acc 0.7906976744186046, correct 34/43\n",
      "class 1: acc 0.675, correct 27/40\n",
      "class 2: acc 0.0, correct 0/25\n",
      "class 3: acc 0.25, correct 3/12\n",
      "class 4: acc 0.13333333333333333, correct 2/15\n",
      "class 5: acc 0.47058823529411764, correct 8/17\n",
      "\n",
      "\n",
      "batch 19, loss: 3.5886, instance_loss: 0.3224, weighted_loss: 1.3023, label: 5, bag_size: 193\n",
      "batch 39, loss: 0.9491, instance_loss: 0.7483, weighted_loss: 0.8085, label: 1, bag_size: 163\n",
      "batch 59, loss: 1.8305, instance_loss: 0.1948, weighted_loss: 0.6855, label: 4, bag_size: 128\n",
      "batch 79, loss: 0.5607, instance_loss: 0.3017, weighted_loss: 0.3794, label: 0, bag_size: 120\n",
      "batch 99, loss: 1.2164, instance_loss: 0.1480, weighted_loss: 0.4685, label: 3, bag_size: 141\n",
      "batch 119, loss: 1.5300, instance_loss: 0.3112, weighted_loss: 0.6769, label: 3, bag_size: 213\n",
      "batch 139, loss: 0.2085, instance_loss: 0.1695, weighted_loss: 0.1812, label: 0, bag_size: 123\n",
      "batch 159, loss: 1.4940, instance_loss: 0.1958, weighted_loss: 0.5853, label: 0, bag_size: 89\n",
      "batch 179, loss: 0.6711, instance_loss: 0.7164, weighted_loss: 0.7029, label: 1, bag_size: 146\n",
      "batch 199, loss: 0.8938, instance_loss: 0.0706, weighted_loss: 0.3175, label: 4, bag_size: 165\n",
      "batch 219, loss: 2.6837, instance_loss: 0.4094, weighted_loss: 1.0917, label: 0, bag_size: 188\n",
      "batch 239, loss: 0.9614, instance_loss: 0.7997, weighted_loss: 0.8482, label: 1, bag_size: 117\n",
      "batch 259, loss: 2.0747, instance_loss: 0.0898, weighted_loss: 0.6853, label: 3, bag_size: 164\n",
      "batch 279, loss: 1.1090, instance_loss: 0.8425, weighted_loss: 0.9225, label: 1, bag_size: 156\n",
      "batch 299, loss: 1.7540, instance_loss: 0.1204, weighted_loss: 0.6105, label: 0, bag_size: 149\n",
      "batch 319, loss: 2.0480, instance_loss: 0.2853, weighted_loss: 0.8141, label: 0, bag_size: 226\n",
      "batch 339, loss: 0.5292, instance_loss: 0.0129, weighted_loss: 0.1678, label: 5, bag_size: 150\n",
      "batch 359, loss: 0.9334, instance_loss: 0.8852, weighted_loss: 0.8997, label: 1, bag_size: 109\n",
      "batch 379, loss: 1.7221, instance_loss: 1.2247, weighted_loss: 1.3739, label: 1, bag_size: 244\n",
      "batch 399, loss: 1.6385, instance_loss: 0.1473, weighted_loss: 0.5946, label: 3, bag_size: 236\n",
      "batch 419, loss: 0.2112, instance_loss: 0.2057, weighted_loss: 0.2074, label: 0, bag_size: 192\n",
      "batch 439, loss: 1.0597, instance_loss: 0.0646, weighted_loss: 0.3631, label: 3, bag_size: 60\n",
      "batch 459, loss: 1.8770, instance_loss: 0.1479, weighted_loss: 0.6666, label: 4, bag_size: 231\n",
      "batch 479, loss: 0.3451, instance_loss: 0.0700, weighted_loss: 0.1525, label: 0, bag_size: 115\n",
      "batch 499, loss: 0.4522, instance_loss: 0.1283, weighted_loss: 0.2255, label: 0, bag_size: 339\n",
      "batch 519, loss: 1.5973, instance_loss: 0.3671, weighted_loss: 0.7362, label: 2, bag_size: 120\n",
      "batch 539, loss: 2.9042, instance_loss: 0.3148, weighted_loss: 1.0916, label: 3, bag_size: 248\n",
      "batch 559, loss: 3.2823, instance_loss: 0.1215, weighted_loss: 1.0697, label: 5, bag_size: 103\n",
      "batch 579, loss: 1.3091, instance_loss: 0.0717, weighted_loss: 0.4429, label: 2, bag_size: 205\n",
      "batch 599, loss: 0.1888, instance_loss: 0.1400, weighted_loss: 0.1547, label: 0, bag_size: 382\n",
      "Epoch: 6, train_loss: 1.0914, train_clustering_loss:  215.0175, train_error: 0.4211\n",
      "class 0: acc 0.7926829268292683, correct 130/164\n",
      "class 1: acc 0.7017543859649122, correct 120/171\n",
      "class 2: acc 0.09722222222222222, correct 7/72\n",
      "class 3: acc 0.3442622950819672, correct 21/61\n",
      "class 4: acc 0.37142857142857144, correct 26/70\n",
      "class 5: acc 0.6857142857142857, correct 48/70\n",
      "\n",
      "Val Set, val_loss: 1.1378, val_error: 0.4737\n",
      "class 0: acc 0.6744186046511628, correct 29/43\n",
      "class 1: acc 0.65, correct 26/40\n",
      "class 2: acc 0.32, correct 8/25\n",
      "class 3: acc 0.16666666666666666, correct 2/12\n",
      "class 4: acc 0.13333333333333333, correct 2/15\n",
      "class 5: acc 0.7647058823529411, correct 13/17\n",
      "\n",
      "\n",
      "batch 19, loss: 1.9619, instance_loss: 0.0165, weighted_loss: 0.6001, label: 4, bag_size: 146\n",
      "batch 39, loss: 0.1136, instance_loss: 0.1485, weighted_loss: 0.1381, label: 0, bag_size: 202\n",
      "batch 59, loss: 1.9645, instance_loss: 0.2008, weighted_loss: 0.7299, label: 3, bag_size: 38\n",
      "batch 79, loss: 0.4086, instance_loss: 0.0752, weighted_loss: 0.1752, label: 0, bag_size: 36\n",
      "batch 99, loss: 0.6750, instance_loss: 0.7026, weighted_loss: 0.6943, label: 1, bag_size: 140\n",
      "batch 119, loss: 0.9751, instance_loss: 0.1037, weighted_loss: 0.3651, label: 3, bag_size: 136\n",
      "batch 139, loss: 0.8683, instance_loss: 0.0191, weighted_loss: 0.2739, label: 4, bag_size: 128\n",
      "batch 159, loss: 0.8037, instance_loss: 0.2618, weighted_loss: 0.4244, label: 0, bag_size: 161\n",
      "batch 179, loss: 1.2313, instance_loss: 0.1489, weighted_loss: 0.4736, label: 2, bag_size: 198\n",
      "batch 199, loss: 0.6460, instance_loss: 0.7978, weighted_loss: 0.7522, label: 1, bag_size: 134\n",
      "batch 219, loss: 1.3988, instance_loss: 0.1871, weighted_loss: 0.5506, label: 2, bag_size: 153\n",
      "batch 239, loss: 1.4977, instance_loss: 0.0463, weighted_loss: 0.4817, label: 2, bag_size: 227\n",
      "batch 259, loss: 1.2871, instance_loss: 0.1596, weighted_loss: 0.4979, label: 4, bag_size: 36\n",
      "batch 279, loss: 1.5805, instance_loss: 0.1255, weighted_loss: 0.5620, label: 2, bag_size: 168\n",
      "batch 299, loss: 0.4539, instance_loss: 0.6108, weighted_loss: 0.5637, label: 1, bag_size: 259\n",
      "batch 319, loss: 1.2541, instance_loss: 1.0396, weighted_loss: 1.1039, label: 1, bag_size: 206\n",
      "batch 339, loss: 2.4409, instance_loss: 0.0931, weighted_loss: 0.7974, label: 2, bag_size: 133\n",
      "batch 359, loss: 0.0979, instance_loss: 0.1949, weighted_loss: 0.1658, label: 0, bag_size: 176\n",
      "batch 379, loss: 1.1540, instance_loss: 0.0566, weighted_loss: 0.3858, label: 4, bag_size: 9\n",
      "batch 399, loss: 0.8937, instance_loss: 0.8106, weighted_loss: 0.8355, label: 1, bag_size: 190\n",
      "batch 419, loss: 1.3299, instance_loss: 0.0038, weighted_loss: 0.4017, label: 4, bag_size: 128\n",
      "batch 439, loss: 3.6689, instance_loss: 0.1636, weighted_loss: 1.2152, label: 4, bag_size: 162\n",
      "batch 459, loss: 0.5897, instance_loss: 0.6859, weighted_loss: 0.6571, label: 1, bag_size: 158\n",
      "batch 479, loss: 0.0940, instance_loss: 0.1505, weighted_loss: 0.1335, label: 0, bag_size: 63\n",
      "batch 499, loss: 0.2564, instance_loss: 0.2086, weighted_loss: 0.2230, label: 0, bag_size: 136\n",
      "batch 519, loss: 0.8533, instance_loss: 0.0884, weighted_loss: 0.3179, label: 0, bag_size: 154\n",
      "batch 539, loss: 2.0650, instance_loss: 0.3544, weighted_loss: 0.8676, label: 2, bag_size: 173\n",
      "batch 559, loss: 0.1377, instance_loss: 0.0047, weighted_loss: 0.0446, label: 5, bag_size: 234\n",
      "batch 579, loss: 1.1338, instance_loss: 0.4250, weighted_loss: 0.6376, label: 2, bag_size: 184\n",
      "batch 599, loss: 0.4442, instance_loss: 0.7470, weighted_loss: 0.6562, label: 1, bag_size: 139\n",
      "Epoch: 7, train_loss: 1.0817, train_clustering_loss:  215.1009, train_error: 0.4145\n",
      "class 0: acc 0.7804878048780488, correct 128/164\n",
      "class 1: acc 0.7309941520467836, correct 125/171\n",
      "class 2: acc 0.1527777777777778, correct 11/72\n",
      "class 3: acc 0.29508196721311475, correct 18/61\n",
      "class 4: acc 0.38571428571428573, correct 27/70\n",
      "class 5: acc 0.6714285714285714, correct 47/70\n",
      "\n",
      "Val Set, val_loss: 1.2359, val_error: 0.5395\n",
      "class 0: acc 0.4883720930232558, correct 21/43\n",
      "class 1: acc 0.875, correct 35/40\n",
      "class 2: acc 0.04, correct 1/25\n",
      "class 3: acc 0.4166666666666667, correct 5/12\n",
      "class 4: acc 0.4, correct 6/15\n",
      "class 5: acc 0.11764705882352941, correct 2/17\n",
      "\n",
      "\n",
      "batch 19, loss: 1.3621, instance_loss: 0.1486, weighted_loss: 0.5127, label: 0, bag_size: 94\n",
      "batch 39, loss: 0.6757, instance_loss: 0.5540, weighted_loss: 0.5905, label: 1, bag_size: 232\n",
      "batch 59, loss: 0.9774, instance_loss: 0.2419, weighted_loss: 0.4626, label: 4, bag_size: 233\n",
      "batch 79, loss: 1.2058, instance_loss: 0.9845, weighted_loss: 1.0509, label: 1, bag_size: 206\n",
      "batch 99, loss: 1.6156, instance_loss: 0.0464, weighted_loss: 0.5172, label: 4, bag_size: 180\n",
      "batch 119, loss: 0.6101, instance_loss: 0.0615, weighted_loss: 0.2261, label: 5, bag_size: 187\n",
      "batch 139, loss: 0.1743, instance_loss: 0.4011, weighted_loss: 0.3331, label: 1, bag_size: 152\n",
      "batch 159, loss: 0.0917, instance_loss: 0.0152, weighted_loss: 0.0382, label: 5, bag_size: 199\n",
      "batch 179, loss: 0.4586, instance_loss: 0.7109, weighted_loss: 0.6352, label: 1, bag_size: 289\n",
      "batch 199, loss: 0.5279, instance_loss: 0.4930, weighted_loss: 0.5034, label: 1, bag_size: 221\n",
      "batch 219, loss: 0.6411, instance_loss: 0.6669, weighted_loss: 0.6591, label: 1, bag_size: 120\n",
      "batch 239, loss: 2.6652, instance_loss: 1.1128, weighted_loss: 1.5785, label: 1, bag_size: 151\n",
      "batch 259, loss: 0.5302, instance_loss: 0.0101, weighted_loss: 0.1661, label: 5, bag_size: 210\n",
      "batch 279, loss: 1.2689, instance_loss: 0.0561, weighted_loss: 0.4199, label: 3, bag_size: 36\n",
      "batch 299, loss: 0.0998, instance_loss: 0.2031, weighted_loss: 0.1721, label: 0, bag_size: 202\n",
      "batch 319, loss: 1.6616, instance_loss: 0.0645, weighted_loss: 0.5436, label: 2, bag_size: 227\n",
      "batch 339, loss: 1.5261, instance_loss: 0.2936, weighted_loss: 0.6633, label: 0, bag_size: 330\n",
      "batch 359, loss: 1.2161, instance_loss: 0.4314, weighted_loss: 0.6668, label: 2, bag_size: 158\n",
      "batch 379, loss: 1.5798, instance_loss: 0.2744, weighted_loss: 0.6661, label: 3, bag_size: 195\n",
      "batch 399, loss: 0.9111, instance_loss: 0.7127, weighted_loss: 0.7722, label: 1, bag_size: 154\n",
      "batch 419, loss: 1.2686, instance_loss: 1.3110, weighted_loss: 1.2983, label: 1, bag_size: 90\n",
      "batch 439, loss: 0.8568, instance_loss: 0.8680, weighted_loss: 0.8646, label: 1, bag_size: 122\n",
      "batch 459, loss: 1.6492, instance_loss: 0.0773, weighted_loss: 0.5489, label: 3, bag_size: 216\n",
      "batch 479, loss: 0.5940, instance_loss: 0.8491, weighted_loss: 0.7726, label: 1, bag_size: 340\n",
      "batch 499, loss: 1.6424, instance_loss: 0.1143, weighted_loss: 0.5727, label: 2, bag_size: 168\n",
      "batch 519, loss: 0.1707, instance_loss: 0.1223, weighted_loss: 0.1368, label: 0, bag_size: 174\n",
      "batch 539, loss: 1.2413, instance_loss: 0.1223, weighted_loss: 0.4580, label: 2, bag_size: 120\n",
      "batch 559, loss: 0.5214, instance_loss: 0.1585, weighted_loss: 0.2674, label: 0, bag_size: 260\n",
      "batch 579, loss: 0.6359, instance_loss: 0.0661, weighted_loss: 0.2370, label: 4, bag_size: 171\n",
      "batch 599, loss: 0.5821, instance_loss: 0.3753, weighted_loss: 0.4373, label: 0, bag_size: 158\n",
      "Epoch: 8, train_loss: 1.0669, train_clustering_loss:  213.7995, train_error: 0.4227\n",
      "class 0: acc 0.7682926829268293, correct 126/164\n",
      "class 1: acc 0.7076023391812866, correct 121/171\n",
      "class 2: acc 0.18055555555555555, correct 13/72\n",
      "class 3: acc 0.2786885245901639, correct 17/61\n",
      "class 4: acc 0.4, correct 28/70\n",
      "class 5: acc 0.6571428571428571, correct 46/70\n",
      "\n",
      "Val Set, val_loss: 1.1728, val_error: 0.5066\n",
      "class 0: acc 0.7906976744186046, correct 34/43\n",
      "class 1: acc 0.55, correct 22/40\n",
      "class 2: acc 0.0, correct 0/25\n",
      "class 3: acc 0.5833333333333334, correct 7/12\n",
      "class 4: acc 0.3333333333333333, correct 5/15\n",
      "class 5: acc 0.4117647058823529, correct 7/17\n",
      "\n",
      "\n",
      "batch 19, loss: 1.9857, instance_loss: 0.1346, weighted_loss: 0.6899, label: 2, bag_size: 85\n",
      "batch 39, loss: 0.7334, instance_loss: 0.8404, weighted_loss: 0.8083, label: 1, bag_size: 222\n",
      "batch 59, loss: 0.7430, instance_loss: 0.2850, weighted_loss: 0.4224, label: 0, bag_size: 119\n",
      "batch 79, loss: 1.1309, instance_loss: 0.0148, weighted_loss: 0.3496, label: 4, bag_size: 167\n",
      "batch 99, loss: 0.9194, instance_loss: 0.0412, weighted_loss: 0.3047, label: 3, bag_size: 200\n",
      "batch 119, loss: 0.9474, instance_loss: 0.0344, weighted_loss: 0.3083, label: 3, bag_size: 160\n",
      "batch 139, loss: 1.9768, instance_loss: 0.1602, weighted_loss: 0.7051, label: 4, bag_size: 315\n",
      "batch 159, loss: 2.0038, instance_loss: 0.1945, weighted_loss: 0.7373, label: 2, bag_size: 654\n",
      "batch 179, loss: 1.5553, instance_loss: 0.9800, weighted_loss: 1.1526, label: 1, bag_size: 141\n",
      "batch 199, loss: 0.6427, instance_loss: 1.0753, weighted_loss: 0.9455, label: 1, bag_size: 169\n",
      "batch 219, loss: 0.8954, instance_loss: 0.2183, weighted_loss: 0.4214, label: 2, bag_size: 205\n",
      "batch 239, loss: 0.1507, instance_loss: 0.1436, weighted_loss: 0.1457, label: 0, bag_size: 216\n",
      "batch 259, loss: 2.4238, instance_loss: 0.0737, weighted_loss: 0.7787, label: 4, bag_size: 205\n",
      "batch 279, loss: 0.1010, instance_loss: 0.1284, weighted_loss: 0.1202, label: 0, bag_size: 158\n",
      "batch 299, loss: 2.7012, instance_loss: 1.3534, weighted_loss: 1.7578, label: 1, bag_size: 90\n",
      "batch 319, loss: 0.0815, instance_loss: 0.1229, weighted_loss: 0.1104, label: 0, bag_size: 103\n",
      "batch 339, loss: 0.7745, instance_loss: 0.1145, weighted_loss: 0.3125, label: 4, bag_size: 158\n",
      "batch 359, loss: 0.4905, instance_loss: 0.6566, weighted_loss: 0.6067, label: 1, bag_size: 235\n",
      "batch 379, loss: 1.5399, instance_loss: 1.1616, weighted_loss: 1.2751, label: 1, bag_size: 196\n",
      "batch 399, loss: 0.4253, instance_loss: 0.6457, weighted_loss: 0.5796, label: 1, bag_size: 163\n",
      "batch 419, loss: 2.7058, instance_loss: 0.0835, weighted_loss: 0.8702, label: 0, bag_size: 144\n",
      "batch 439, loss: 1.6457, instance_loss: 0.1912, weighted_loss: 0.6276, label: 3, bag_size: 202\n",
      "batch 459, loss: 1.0989, instance_loss: 0.9914, weighted_loss: 1.0237, label: 1, bag_size: 134\n",
      "batch 479, loss: 3.2277, instance_loss: 0.3687, weighted_loss: 1.2264, label: 5, bag_size: 159\n",
      "batch 499, loss: 0.4965, instance_loss: 0.9894, weighted_loss: 0.8415, label: 1, bag_size: 214\n",
      "batch 519, loss: 1.0600, instance_loss: 1.0071, weighted_loss: 1.0230, label: 1, bag_size: 122\n",
      "batch 539, loss: 0.3508, instance_loss: 0.0301, weighted_loss: 0.1263, label: 5, bag_size: 176\n",
      "batch 559, loss: 0.9356, instance_loss: 0.0398, weighted_loss: 0.3086, label: 3, bag_size: 145\n",
      "batch 579, loss: 0.4834, instance_loss: 0.5494, weighted_loss: 0.5296, label: 1, bag_size: 212\n",
      "batch 599, loss: 0.3660, instance_loss: 0.6108, weighted_loss: 0.5374, label: 1, bag_size: 116\n",
      "Epoch: 9, train_loss: 1.0113, train_clustering_loss:  207.0893, train_error: 0.3931\n",
      "class 0: acc 0.8170731707317073, correct 134/164\n",
      "class 1: acc 0.6608187134502924, correct 113/171\n",
      "class 2: acc 0.3055555555555556, correct 22/72\n",
      "class 3: acc 0.3114754098360656, correct 19/61\n",
      "class 4: acc 0.5142857142857142, correct 36/70\n",
      "class 5: acc 0.6428571428571429, correct 45/70\n",
      "\n",
      "Val Set, val_loss: 1.1569, val_error: 0.5197\n",
      "class 0: acc 0.7674418604651163, correct 33/43\n",
      "class 1: acc 0.525, correct 21/40\n",
      "class 2: acc 0.08, correct 2/25\n",
      "class 3: acc 0.3333333333333333, correct 4/12\n",
      "class 4: acc 0.13333333333333333, correct 2/15\n",
      "class 5: acc 0.6470588235294118, correct 11/17\n"
     ]
    }
   ],
   "source": [
    "from deep_learning.loaders.feature_dataset import FeatureDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from deep_learning.train.train import train\n",
    "from deep_learning.models.attention_core.clam import Clam\n",
    "\n",
    "df=pd.read_csv(\"F:/data/small.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "model=Clam(\n",
    "    feature_vector_length=2048,\n",
    "    dropout=0.1,\n",
    "    k_sample=8,\n",
    "    n_classes=6,\n",
    "    subtyping=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train(model,train_df,\"F:/extracted_features\",10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
