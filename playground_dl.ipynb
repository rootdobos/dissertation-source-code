{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([200, 2048, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from deep_learning.models.feature_extractors.resnet import Resnet50\n",
    "\n",
    "model= Resnet50()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "random_input = torch.randn(200, 3, 256, 256).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    features = model(random_input)\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.julia\\conda\\3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\.julia\\conda\\3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B1_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B1_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([200, 1280, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from deep_learning.models.feature_extractors.efficientnet import EfficientNetB1\n",
    "\n",
    "model= EfficientNetB1()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "random_input = torch.randn(200, 3, 256, 256).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    features = model(random_input)\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.julia\\conda\\3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\.julia\\conda\\3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from deep_learning.feature_extraction import extract_features\n",
    "from deep_learning.models.transforms.transforms import transform_resnet\n",
    "from deep_learning.models.feature_extractors.resnet import Resnet50\n",
    "from deep_learning.loaders.image_batch_loader import load_batch_from_dir\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=Resnet50().to(device)\n",
    "\n",
    "features=extract_features(model,transform_resnet,\"F:/test/0005f7aaab2800f6170c399693a96917/tiles\")\n",
    "torch.save(features,\"F:/test/0005f7aaab2800f6170c399693a96917.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'features/img1.npy'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame (assuming it’s loaded from a CSV)\n",
    "df = pd.DataFrame({\n",
    "    \"feature_path\": [\"features/img1.npy\", \"features/img2.npy\", \"features/img3.npy\"],\n",
    "    \"label\": [0, 1, 0]\n",
    "})\n",
    "df[\"feature_path\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 760\n",
      "Test size: 191\n",
      "Train batches: 760\n",
      "Batch X shape: torch.Size([193, 2048])\n",
      "Batch Y shape: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "from deep_learning.loaders.feature_dataset import FeatureDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "df=pd.read_csv(\"F:/data/small.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Test size:\", len(test_df))\n",
    "\n",
    "dataset=FeatureDataset(train_df,\"F:/extracted_features\")\n",
    "\n",
    "train_loader = DataLoader(dataset, shuffle=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "\n",
    "for batch in train_loader:\n",
    "    x_batch, y_batch = batch\n",
    "    x_batch=x_batch.squeeze()\n",
    "    print(\"Batch X shape:\", x_batch.shape)  # Expected: (32, D)\n",
    "    print(\"Batch Y shape:\", y_batch.squeeze().shape)  # Expected: (32,)\n",
    "    break  # Only one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Clam                                     [1, 5]                    5,130\n",
       "├─Sequential: 1-1                        [100, 5]                  --\n",
       "│    └─Linear: 2-1                       [100, 512]                1,049,088\n",
       "│    └─ReLU: 2-2                         [100, 512]                --\n",
       "│    └─Dropout: 2-3                      [100, 512]                --\n",
       "│    └─Attn_Net_Gated: 2-4               [100, 5]                  --\n",
       "│    │    └─Sequential: 3-1              [100, 256]                131,328\n",
       "│    │    └─Sequential: 3-2              [100, 256]                131,328\n",
       "│    │    └─Linear: 3-3                  [100, 5]                  1,285\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─Linear: 2-5                       [1]                       513\n",
       "│    └─Linear: 2-6                       [1]                       513\n",
       "│    └─Linear: 2-7                       [1]                       513\n",
       "│    └─Linear: 2-8                       [1]                       513\n",
       "│    └─Linear: 2-9                       [1]                       513\n",
       "==========================================================================================\n",
       "Total params: 1,320,724\n",
       "Trainable params: 1,320,724\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 131.31\n",
       "==========================================================================================\n",
       "Input size (MB): 0.82\n",
       "Forward/backward pass size (MB): 0.82\n",
       "Params size (MB): 5.26\n",
       "Estimated Total Size (MB): 6.90\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deep_learning.models.attention_core.clam import Clam\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=Clam(\n",
    "    feature_vector_length=2048,\n",
    "    dropout=0.1,\n",
    "    k_sample=8,\n",
    "    n_classes=5,\n",
    "    subtyping=True\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "summary(model, input_size=(100,2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0363, -0.0058,  0.0846, -0.0593,  0.0797]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    x_batch, y_batch = batch\n",
    "    x_batch=x_batch.squeeze()\n",
    "    x_batch.to(device)\n",
    "    logits, Y_prob, Y_hat, A_raw, results_dict=model(x_batch)\n",
    "    print(logits)\n",
    "    break  # Only one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "batch 19, loss: 2.6146, instance_loss: 0.7178, weighted_loss: 1.6662, label: 2, bag_size: 198\n",
      "batch 39, loss: 1.7910, instance_loss: 0.6853, weighted_loss: 1.2382, label: 5, bag_size: 145\n",
      "batch 59, loss: 2.4337, instance_loss: 0.7029, weighted_loss: 1.5683, label: 3, bag_size: 128\n",
      "batch 79, loss: 1.0695, instance_loss: 0.5800, weighted_loss: 0.8247, label: 1, bag_size: 395\n",
      "batch 99, loss: 3.1832, instance_loss: 0.6473, weighted_loss: 1.9152, label: 2, bag_size: 184\n",
      "batch 119, loss: 1.7686, instance_loss: 0.5912, weighted_loss: 1.1799, label: 1, bag_size: 246\n",
      "batch 139, loss: 2.1358, instance_loss: 0.6433, weighted_loss: 1.3895, label: 3, bag_size: 167\n",
      "batch 159, loss: 1.2676, instance_loss: 0.3248, weighted_loss: 0.7962, label: 0, bag_size: 236\n",
      "batch 179, loss: 2.4001, instance_loss: 0.8914, weighted_loss: 1.6457, label: 5, bag_size: 134\n",
      "batch 199, loss: 2.4610, instance_loss: 0.6065, weighted_loss: 1.5337, label: 2, bag_size: 225\n",
      "batch 219, loss: 1.2204, instance_loss: 0.5954, weighted_loss: 0.9079, label: 1, bag_size: 251\n",
      "batch 239, loss: 1.2330, instance_loss: 0.1758, weighted_loss: 0.7044, label: 0, bag_size: 208\n",
      "batch 259, loss: 0.9294, instance_loss: 0.2110, weighted_loss: 0.5702, label: 1, bag_size: 166\n",
      "batch 279, loss: 2.5239, instance_loss: 0.5973, weighted_loss: 1.5606, label: 2, bag_size: 290\n",
      "batch 299, loss: 1.2293, instance_loss: 0.2009, weighted_loss: 0.7151, label: 1, bag_size: 201\n",
      "batch 319, loss: 1.8755, instance_loss: 0.7739, weighted_loss: 1.3247, label: 0, bag_size: 89\n",
      "batch 339, loss: 2.3296, instance_loss: 0.5998, weighted_loss: 1.4647, label: 2, bag_size: 186\n",
      "batch 359, loss: 1.1174, instance_loss: 0.0736, weighted_loss: 0.5955, label: 1, bag_size: 151\n",
      "batch 379, loss: 1.8497, instance_loss: 0.2124, weighted_loss: 1.0311, label: 3, bag_size: 150\n",
      "batch 399, loss: 2.3139, instance_loss: 0.3192, weighted_loss: 1.3165, label: 4, bag_size: 154\n",
      "batch 419, loss: 1.1358, instance_loss: 0.0632, weighted_loss: 0.5995, label: 1, bag_size: 148\n",
      "batch 439, loss: 2.2055, instance_loss: 0.1915, weighted_loss: 1.1985, label: 4, bag_size: 315\n",
      "batch 459, loss: 1.0940, instance_loss: 0.0345, weighted_loss: 0.5643, label: 1, bag_size: 259\n",
      "batch 479, loss: 1.7632, instance_loss: 0.3537, weighted_loss: 1.0584, label: 5, bag_size: 187\n",
      "batch 499, loss: 2.1493, instance_loss: 0.2486, weighted_loss: 1.1990, label: 5, bag_size: 234\n",
      "batch 519, loss: 1.4090, instance_loss: 1.1365, weighted_loss: 1.2728, label: 5, bag_size: 113\n",
      "batch 539, loss: 1.3623, instance_loss: 0.2935, weighted_loss: 0.8279, label: 0, bag_size: 147\n",
      "batch 559, loss: 1.6447, instance_loss: 0.1355, weighted_loss: 0.8901, label: 4, bag_size: 185\n",
      "batch 579, loss: 2.0042, instance_loss: 0.3067, weighted_loss: 1.1555, label: 2, bag_size: 176\n",
      "batch 599, loss: 0.8147, instance_loss: 0.0434, weighted_loss: 0.4290, label: 0, bag_size: 155\n",
      "Epoch 0 Summary: train_loss: 1.6744, train_clustering_loss:  0.4316, train_accuracy: 0.2549\n",
      "Slide Accuracy:\n",
      "class 0: acc 0.39634146341463417, correct 65/164\n",
      "class 1: acc 0.42105263157894735, correct 72/171\n",
      "class 2: acc 0.027777777777777776, correct 2/72\n",
      "class 3: acc 0.0, correct 0/61\n",
      "class 4: acc 0.05714285714285714, correct 4/70\n",
      "class 5: acc 0.17142857142857143, correct 12/70\n",
      "Patch Accuracy:\n",
      "class 0: acc 0.8715049342105263, correct 4239/4864\n",
      "class 1: acc 0.7569901315789473, correct 3682/4864\n",
      "class 2: acc None, correct 0/0\n",
      "class 3: acc None, correct 0/0\n",
      "class 4: acc None, correct 0/0\n",
      "class 5: acc None, correct 0/0\n",
      "[[65. 78. 11.  1.  2.  7.]\n",
      " [76. 72. 11.  1.  3.  8.]\n",
      " [28. 37.  2.  0.  1.  4.]\n",
      " [13. 38.  3.  0.  2.  5.]\n",
      " [13. 39.  3.  0.  4. 11.]\n",
      " [23. 30.  2.  0.  3. 12.]]\n",
      "\n",
      "\n",
      "batch 19, loss: 1.7901, instance_loss: 0.6936, weighted_loss: 1.2418, label: 1, bag_size: 90\n",
      "batch 39, loss: 2.1667, instance_loss: 0.2273, weighted_loss: 1.1970, label: 2, bag_size: 153\n",
      "batch 59, loss: 1.2636, instance_loss: 0.9117, weighted_loss: 1.0876, label: 4, bag_size: 9\n",
      "batch 79, loss: 0.8532, instance_loss: 0.0343, weighted_loss: 0.4437, label: 0, bag_size: 180\n",
      "batch 99, loss: 4.7571, instance_loss: 0.2322, weighted_loss: 2.4947, label: 5, bag_size: 266\n",
      "batch 119, loss: 1.3997, instance_loss: 0.0329, weighted_loss: 0.7163, label: 1, bag_size: 212\n",
      "batch 139, loss: 0.9372, instance_loss: 0.7659, weighted_loss: 0.8516, label: 5, bag_size: 152\n",
      "batch 159, loss: 1.1064, instance_loss: 0.0280, weighted_loss: 0.5672, label: 1, bag_size: 291\n",
      "batch 179, loss: 2.7680, instance_loss: 0.5752, weighted_loss: 1.6716, label: 5, bag_size: 197\n",
      "batch 199, loss: 1.8825, instance_loss: 0.3852, weighted_loss: 1.1338, label: 1, bag_size: 164\n",
      "batch 219, loss: 1.3357, instance_loss: 0.0149, weighted_loss: 0.6753, label: 1, bag_size: 221\n",
      "batch 239, loss: 2.8108, instance_loss: 0.1165, weighted_loss: 1.4636, label: 3, bag_size: 162\n",
      "batch 259, loss: 2.4455, instance_loss: 0.5041, weighted_loss: 1.4748, label: 5, bag_size: 126\n",
      "batch 279, loss: 0.5323, instance_loss: 0.0486, weighted_loss: 0.2904, label: 0, bag_size: 89\n",
      "batch 299, loss: 0.8538, instance_loss: 0.0951, weighted_loss: 0.4745, label: 0, bag_size: 214\n",
      "batch 319, loss: 2.2592, instance_loss: 0.3092, weighted_loss: 1.2842, label: 2, bag_size: 175\n",
      "batch 339, loss: 1.0405, instance_loss: 0.0614, weighted_loss: 0.5509, label: 1, bag_size: 159\n",
      "batch 359, loss: 0.8743, instance_loss: 0.1382, weighted_loss: 0.5062, label: 1, bag_size: 141\n",
      "batch 379, loss: 1.1449, instance_loss: 0.0339, weighted_loss: 0.5894, label: 1, bag_size: 222\n",
      "batch 399, loss: 1.5279, instance_loss: 0.2104, weighted_loss: 0.8691, label: 5, bag_size: 158\n",
      "batch 419, loss: 2.0672, instance_loss: 0.3214, weighted_loss: 1.1943, label: 2, bag_size: 186\n",
      "batch 439, loss: 0.9771, instance_loss: 0.1094, weighted_loss: 0.5432, label: 0, bag_size: 170\n",
      "batch 459, loss: 0.9397, instance_loss: 0.0227, weighted_loss: 0.4812, label: 1, bag_size: 193\n",
      "batch 479, loss: 3.0415, instance_loss: 0.0792, weighted_loss: 1.5603, label: 3, bag_size: 216\n",
      "batch 499, loss: 1.1370, instance_loss: 0.3001, weighted_loss: 0.7185, label: 1, bag_size: 125\n",
      "batch 519, loss: 2.4884, instance_loss: 0.6609, weighted_loss: 1.5747, label: 2, bag_size: 195\n",
      "batch 539, loss: 1.8042, instance_loss: 0.2967, weighted_loss: 1.0505, label: 3, bag_size: 211\n",
      "batch 559, loss: 2.0103, instance_loss: 0.2645, weighted_loss: 1.1374, label: 2, bag_size: 184\n",
      "batch 579, loss: 1.4322, instance_loss: 0.1993, weighted_loss: 0.8157, label: 1, bag_size: 141\n",
      "batch 599, loss: 0.8992, instance_loss: 0.1361, weighted_loss: 0.5176, label: 1, bag_size: 114\n",
      "Epoch 1 Summary: train_loss: 1.5550, train_clustering_loss:  0.2799, train_accuracy: 0.3487\n",
      "Slide Accuracy:\n",
      "class 0: acc 0.49390243902439024, correct 81/164\n",
      "class 1: acc 0.5029239766081871, correct 86/171\n",
      "class 2: acc 0.027777777777777776, correct 2/72\n",
      "class 3: acc 0.03278688524590164, correct 2/61\n",
      "class 4: acc 0.18571428571428572, correct 13/70\n",
      "class 5: acc 0.4, correct 28/70\n",
      "Patch Accuracy:\n",
      "class 0: acc 0.9076891447368421, correct 4415/4864\n",
      "class 1: acc 0.8682154605263158, correct 4223/4864\n",
      "class 2: acc None, correct 0/0\n",
      "class 3: acc None, correct 0/0\n",
      "class 4: acc None, correct 0/0\n",
      "class 5: acc None, correct 0/0\n",
      "[[81. 57.  5.  5. 12.  4.]\n",
      " [63. 86.  5.  1. 11.  5.]\n",
      " [18. 40.  2.  2.  7.  3.]\n",
      " [ 9. 29.  3.  2. 11.  7.]\n",
      " [15. 16.  4.  3. 13. 19.]\n",
      " [14. 13.  1.  4. 10. 28.]]\n",
      "\n",
      "\n",
      "batch 19, loss: 0.9869, instance_loss: 0.0368, weighted_loss: 0.5119, label: 1, bag_size: 275\n",
      "batch 39, loss: 2.0860, instance_loss: 0.1016, weighted_loss: 1.0938, label: 4, bag_size: 187\n",
      "batch 59, loss: 1.7602, instance_loss: 0.0513, weighted_loss: 0.9058, label: 4, bag_size: 160\n",
      "batch 79, loss: 1.4759, instance_loss: 0.1233, weighted_loss: 0.7996, label: 1, bag_size: 188\n",
      "batch 99, loss: 1.4619, instance_loss: 0.6477, weighted_loss: 1.0548, label: 3, bag_size: 115\n",
      "batch 119, loss: 0.8315, instance_loss: 0.2265, weighted_loss: 0.5290, label: 0, bag_size: 257\n",
      "batch 139, loss: 0.4347, instance_loss: 0.1691, weighted_loss: 0.3019, label: 0, bag_size: 66\n",
      "batch 159, loss: 0.5454, instance_loss: 0.0564, weighted_loss: 0.3009, label: 5, bag_size: 150\n",
      "batch 179, loss: 0.7364, instance_loss: 0.1091, weighted_loss: 0.4227, label: 0, bag_size: 104\n",
      "batch 199, loss: 1.4602, instance_loss: 0.4309, weighted_loss: 0.9456, label: 4, bag_size: 242\n",
      "batch 219, loss: 2.3760, instance_loss: 0.1292, weighted_loss: 1.2526, label: 2, bag_size: 123\n",
      "batch 239, loss: 0.6842, instance_loss: 0.1169, weighted_loss: 0.4005, label: 0, bag_size: 139\n",
      "batch 259, loss: 2.2720, instance_loss: 0.1673, weighted_loss: 1.2197, label: 2, bag_size: 147\n",
      "batch 279, loss: 2.0188, instance_loss: 0.3473, weighted_loss: 1.1831, label: 3, bag_size: 188\n",
      "batch 299, loss: 1.0261, instance_loss: 0.0370, weighted_loss: 0.5316, label: 5, bag_size: 125\n",
      "batch 319, loss: 1.1822, instance_loss: 0.1723, weighted_loss: 0.6772, label: 3, bag_size: 145\n",
      "batch 339, loss: 1.6081, instance_loss: 0.3703, weighted_loss: 0.9892, label: 0, bag_size: 36\n",
      "batch 359, loss: 1.6310, instance_loss: 0.0766, weighted_loss: 0.8538, label: 5, bag_size: 57\n",
      "batch 379, loss: 0.9923, instance_loss: 0.0125, weighted_loss: 0.5024, label: 1, bag_size: 302\n",
      "batch 399, loss: 1.0527, instance_loss: 0.1498, weighted_loss: 0.6012, label: 0, bag_size: 240\n",
      "batch 419, loss: 1.1731, instance_loss: 0.1532, weighted_loss: 0.6631, label: 0, bag_size: 196\n",
      "batch 439, loss: 0.4995, instance_loss: 0.0225, weighted_loss: 0.2610, label: 1, bag_size: 395\n",
      "batch 459, loss: 2.1368, instance_loss: 0.0807, weighted_loss: 1.1087, label: 0, bag_size: 103\n",
      "batch 479, loss: 0.8104, instance_loss: 0.0194, weighted_loss: 0.4149, label: 1, bag_size: 235\n",
      "batch 499, loss: 2.0539, instance_loss: 0.2329, weighted_loss: 1.1434, label: 2, bag_size: 184\n",
      "batch 519, loss: 5.8239, instance_loss: 0.0807, weighted_loss: 2.9523, label: 3, bag_size: 150\n",
      "batch 539, loss: 0.8892, instance_loss: 0.0652, weighted_loss: 0.4772, label: 0, bag_size: 154\n",
      "batch 559, loss: 0.6772, instance_loss: 0.0574, weighted_loss: 0.3673, label: 0, bag_size: 214\n",
      "batch 579, loss: 2.1410, instance_loss: 0.4939, weighted_loss: 1.3175, label: 4, bag_size: 40\n",
      "batch 599, loss: 1.0773, instance_loss: 0.0737, weighted_loss: 0.5755, label: 0, bag_size: 153\n",
      "Epoch 2 Summary: train_loss: 1.4895, train_clustering_loss:  0.1997, train_accuracy: 0.3914\n",
      "Slide Accuracy:\n",
      "class 0: acc 0.5792682926829268, correct 95/164\n",
      "class 1: acc 0.52046783625731, correct 89/171\n",
      "class 2: acc 0.0, correct 0/72\n",
      "class 3: acc 0.11475409836065574, correct 7/61\n",
      "class 4: acc 0.17142857142857143, correct 12/70\n",
      "class 5: acc 0.5, correct 35/70\n",
      "Patch Accuracy:\n",
      "class 0: acc 0.9391447368421053, correct 4568/4864\n",
      "class 1: acc 0.9159128289473685, correct 4455/4864\n",
      "class 2: acc None, correct 0/0\n",
      "class 3: acc None, correct 0/0\n",
      "class 4: acc None, correct 0/0\n",
      "class 5: acc None, correct 0/0\n",
      "[[95. 51.  0.  1.  9.  8.]\n",
      " [68. 89.  0.  7.  6.  1.]\n",
      " [18. 38.  0.  4.  7.  5.]\n",
      " [ 9. 18.  0.  7. 13. 14.]\n",
      " [10. 25.  0.  7. 12. 16.]\n",
      " [11. 10.  0.  7.  7. 35.]]\n"
     ]
    }
   ],
   "source": [
    "from deep_learning.loaders.feature_dataset import FeatureDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from deep_learning.train.train import train\n",
    "from deep_learning.models.attention_core.clam import Clam\n",
    "\n",
    "df=pd.read_csv(\"F:/data/small.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "model=Clam(\n",
    "    feature_vector_length=2048,\n",
    "    dropout=0.1,\n",
    "    k_sample=8,\n",
    "    n_classes=6,\n",
    "    subtyping=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train(model,train_df,\"F:/extracted_features\",3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadratic Weighted Kappa: 0.9383\n",
      "tensor([50, 45, 40, 50, 45, 47], dtype=torch.int32)\n",
      "tensor([ 4,  7, 12, 17,  9,  9])\n",
      "tensor([ 4, 11, 14,  9, 13,  7])\n",
      "(tensor([0.9259, 0.8654, 0.7692, 0.7463, 0.8333, 0.8393]), tensor([0.9259, 0.8036, 0.7407, 0.8475, 0.7759, 0.8704]), tensor([0.9259, 0.8333, 0.7547, 0.7937, 0.8036, 0.8545]), 0.8276239037513733, 0.8268638253211975)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from deep_learning.utils.metrics import quadratic_weighted_kappa,compute_precision_recall_f1\n",
    "y_true = np.array([0, 1, 2, 2, 3, 3, 4, 4, 5, 5]) \n",
    "y_pred = np.array([0, 1, 2, 3, 3, 2, 4, 3, 5, 5])\n",
    "\n",
    "qwk_score = quadratic_weighted_kappa(y_true=y_true, y_pred=y_pred, num_classes=6)\n",
    "print(f\"Quadratic Weighted Kappa: {qwk_score:.4f}\")\n",
    "conf_matrix = np.array([\n",
    "    [50,  2,  1,  0,  0,  1],  # Class 0\n",
    "    [ 3, 45,  5,  2,  1,  0],  # Class 1\n",
    "    [ 0,  4, 40,  7,  2,  1],  # Class 2\n",
    "    [ 0,  0,  3, 50,  4,  2],  # Class 3\n",
    "    [ 0,  1,  2,  5, 45,  5],  # Class 4\n",
    "    [ 1,  0,  1,  3,  2, 47]   # Class 5\n",
    "])\n",
    "print(compute_precision_recall_f1(conf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:tensor([4])\n",
      "Y_hat: 4, Y_prob: ['0.0000', '0.0001', '0.0031', '0.1960', '0.5492', '0.2516']\n",
      "tensor([[2.5718e-05, 7.2330e-05, 3.1172e-03, 1.9599e-01, 5.4920e-01, 2.5159e-01]],\n",
      "       device='cuda:0')\n",
      "3    72\n",
      "5    46\n",
      "0    43\n",
      "4    11\n",
      "1     6\n",
      "2     1\n",
      "Name: prediction, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from deep_learning.loaders.feature_dataset import FeatureDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from deep_learning.models.attention_core.clam import Clam\n",
    "from deep_learning.services.attention_score_service import get_best_attention_scores\n",
    "df=pd.read_csv(\"F:/data/train_stratified.csv\")\n",
    "\n",
    "model=Clam(\n",
    "    feature_vector_length=2048,\n",
    "    dropout=0.1,\n",
    "    k_sample=8,\n",
    "    n_classes=6,\n",
    "    subtyping=False\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(\"F:/logs/resnet_clam/20250306_220754_p7_b75/model_weights_14.pth\"))\n",
    "\n",
    "df=pd.read_csv(\"F:/data/train_stratified.csv\")\n",
    "feature_path=\"F:/extracted_features_resnet\"\n",
    "dataset=FeatureDataset(df,feature_path)\n",
    "data_loader=DataLoader(dataset, batch_size=1, shuffle=True,num_workers=0)\n",
    "features,label=next(iter(data_loader))\n",
    "\n",
    "print(f\"Label:{label}\")\n",
    "best_scores=get_best_attention_scores(model, features);\n",
    "score_df=pd.DataFrame(best_scores)\n",
    "cat_counts=score_df[\"prediction\"].value_counts()\n",
    "print(cat_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_hat: 5, Y_prob: ['0.0012', '0.0002', '0.0001', '0.0039', '0.0947', '0.8998']\n",
      "tensor([[1.2234e-03, 1.7128e-04, 1.0321e-04, 3.9496e-03, 9.4715e-02, 8.9984e-01]],\n",
      "       device='cuda:0')\n",
      "3    80\n",
      "4     4\n",
      "0     3\n",
      "2     3\n",
      "5     1\n",
      "Name: predicted_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from deep_learning.models.attention_core.clam import Clam\n",
    "from deep_learning.services.attention_score_service import (get_best_attention_scores,\n",
    "                               get_tiles_coords,pair_coords_and_attention_scores,get_min_max_coordinates)\n",
    "\n",
    "from deep_learning.services.file_and_data_service import FileDataService\n",
    "from deep_learning.services.attention_visualization_service import ImageComposer\n",
    "model=Clam(\n",
    "    feature_vector_length=2048,\n",
    "    dropout=0.1,\n",
    "    k_sample=8,\n",
    "    n_classes=6,\n",
    "    subtyping=False\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(\"F:/logs/resnet_clam/20250306_220754_p7_b75/model_weights_14.pth\"))\n",
    "\n",
    "slide_name=\"1d3d8045a44963e4367b437fdf361dc5\"\n",
    "feature_path=\"F:/extracted_features_resnet\"\n",
    "fd=FileDataService(feature_path)\n",
    "tiles_path=f\"F:/extracted/{slide_name}/tiles\"\n",
    "\n",
    "output_path=f\"F:/output/{slide_name}\"\n",
    "\n",
    "#Data Preparation\n",
    "features=fd.load_image_features(slide_name)\n",
    "\n",
    "best_scores=get_best_attention_scores(model, features)\n",
    "coords=get_tiles_coords(tiles_path)\n",
    "data=pair_coords_and_attention_scores(best_scores,coords)\n",
    "min_max_coords=get_min_max_coordinates(data)\n",
    "\n",
    "img_composer=ImageComposer(256,min_max_coords,data)\n",
    "img_composer.create_composed_images(tiles_path,output_path)\n",
    "\n",
    "score_df=pd.DataFrame(best_scores)\n",
    "cat_counts=score_df[\"predicted_label\"].value_counts()\n",
    "print(cat_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_hat: 1, Y_prob: ['0.1931', '0.5989', '0.1644', '0.0364', '0.0017', '0.0056']\n",
      "tensor([[0.1931, 0.5989, 0.1644, 0.0364, 0.0017, 0.0056]], device='cuda:0')\n",
      "5    160\n",
      "4      7\n",
      "0      6\n",
      "3      1\n",
      "Name: predicted_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from deep_learning.models.attention_core.clam import Clam\n",
    "from deep_learning.services.attention_score_service import (get_best_attention_scores,\n",
    "                               get_tiles_coords,pair_coords_and_attention_scores,get_min_max_coordinates)\n",
    "\n",
    "from deep_learning.services.file_and_data_service import FileDataService\n",
    "from deep_learning.services.attention_visualization_service import ImageComposer\n",
    "model=Clam(\n",
    "    feature_vector_length=1280,\n",
    "    dropout=0.1,\n",
    "    k_sample=8,\n",
    "    n_classes=6,\n",
    "    subtyping=False\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(\"F:/logs/effnet_clam/20250306_223024_p7_b75/model_weights_4.pth\"))\n",
    "\n",
    "slide_name=\"c071c418f185ef8191b4eddb6efcc316\"\n",
    "feature_path=\"F:/extracted_features_effnet\"\n",
    "fd=FileDataService(feature_path)\n",
    "tiles_path=f\"F:/extracted/{slide_name}/tiles\"\n",
    "\n",
    "output_path=f\"F:/output_effnet/{slide_name}\"\n",
    "\n",
    "#Data Preparation\n",
    "features=fd.load_image_features(slide_name)\n",
    "\n",
    "best_scores=get_best_attention_scores(model, features)\n",
    "coords=get_tiles_coords(tiles_path)\n",
    "data=pair_coords_and_attention_scores(best_scores,coords)\n",
    "min_max_coords=get_min_max_coordinates(data)\n",
    "\n",
    "img_composer=ImageComposer(256,min_max_coords,data)\n",
    "img_composer.create_composed_images(tiles_path,output_path)\n",
    "\n",
    "score_df=pd.DataFrame(best_scores)\n",
    "cat_counts=score_df[\"predicted_label\"].value_counts()\n",
    "print(cat_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
